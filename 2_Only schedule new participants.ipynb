{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final run for the Atlanta Writer's Conference Scheduling\n",
    "*Becky Hodge*\n",
    "\n",
    "#### Summary\n",
    "We previously ran all the code to schedule participants, etc. However, there are always changes and drop-outs and additions to teh waitlist, etc. after we send the email one month prior to the conference telling people their schedules.\n",
    "\n",
    "In this code, we will rerun a lot of the prior code, but specifically ONLY for participants who had any changes made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and clean the different files/reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Install any needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os \n",
    "\n",
    "today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Set the conference dates\n",
    "date_str_fri = '2025-05-02'\n",
    "date_str_sat = '2025-05-03'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Select the file with the most recent date\n",
    "directory = 'May2025_reports'\n",
    "\n",
    "most_recent_file = max(\n",
    "    (f for f in os.listdir(directory) if f.startswith('Registered_') and f.endswith('.csv')),\n",
    "    key=lambda x: datetime.datetime.strptime(x.split('_')[1].split('.')[0], '%m-%d-%y'),\n",
    ")\n",
    "\n",
    "# Load the most recent file\n",
    "most_recent_path = os.path.join(directory, most_recent_file)\n",
    "registered = pd.read_csv(most_recent_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "most_recent_file = max(\n",
    "    (f for f in os.listdir(directory) if f.startswith('Waitlists_') and f.endswith('.csv')),\n",
    "    key=lambda x: datetime.datetime.strptime(x.split('_')[1].split('.')[0], '%m-%d-%y'),\n",
    ")\n",
    "\n",
    "# Load the most recent file\n",
    "most_recent_path = os.path.join(directory, most_recent_file)\n",
    "waitlist = pd.read_csv(most_recent_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code brings in ALL participants, which is key for knowing whether any waitlist only people are virtual or in person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "most_recent_file = max(\n",
    "    (f for f in os.listdir(directory) if f.startswith('Allparticipants_') and f.endswith('.csv')),\n",
    "    key=lambda x: datetime.datetime.strptime(x.split('_')[1].split('.')[0], '%m-%d-%y'),\n",
    ")\n",
    "\n",
    "# Load the most recent file\n",
    "most_recent_path = os.path.join(directory, most_recent_file)\n",
    "all_participants = pd.read_csv(most_recent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "all_participants = all_participants.rename(columns={'Email Address':'Email'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Filter this dataset to just virtual people\n",
    "virtual_only = all_participants.loc[all_participants['Hotel vs. Zoom'] == 'Virtually via Zoom (only available for query letter critiques, manuscript sample critiques, and pitches)', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(directory, most_recent_file, most_recent_path)\n",
    "\n",
    "fict_gen = pd.read_excel('List_of_genres_agents_editors.xlsx', sheet_name='fiction')\n",
    "nonfict_gen = pd.read_excel('List_of_genres_agents_editors.xlsx', sheet_name='nonfiction')\n",
    "pubs = pd.read_excel('List_of_genres_agents_editors.xlsx', sheet_name='agents_editors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh gosh, some of the column names are hefty...  Let's fix those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "registered = registered.rename(columns={'Hotel vs. Zoom':'Virtual', \n",
    "                                        \"What fiction genre(s) will you be presenting to agents/editors at the conference? (If you're not signing up for any agent/editor meetings, indicate which genre(s) you write.)\":'Fiction genre', \n",
    "                                        \"What nonfiction topic(s) will you be presenting to agents/editors at the conference? (If you're not signing up for any agent/editor meetings, indicate which topic(s) you write.)\":'Nonfiction genre', \n",
    "                                        'Registration Date (GMT)':'Registration Date',\n",
    "                                        'Email Address':'Email'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "waitlist = waitlist.rename(columns={'Registration Date (GMT)':'Registration Date',\n",
    "                                     'Email Address':'Email'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also fix so that we drop the 'Not applicable --I don't write fiction' and 'Not applicable--I don't write nonfiction'. We'll set them to missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "registered['Fiction genre']= registered['Fiction genre'].replace(\"Not Applicable --I don't write fiction\", np.nan)\n",
    "registered['Nonfiction genre']= registered['Nonfiction genre'].replace(\"Not Applicable--I don't write nonfiction\", np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, there's people who wrote in 'Other',  but for our purposes, we don't care about that info for the purposes of matching to agents/editors. Let's remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "def clean_genres(genre_string):\n",
    "    if genre_string is None or pd.isna(genre_string) or \"\":\n",
    "        return \"\"\n",
    "\n",
    "    genres = [genre.strip() for genre in genre_string.split(',')]\n",
    "    cleaned_genres = [genre for genre in genres if not re.match(r\"^Other \\(please specify\\):\", genre)]\n",
    "\n",
    "    return \", \".join(cleaned_genres)\n",
    "\n",
    "registered['Fiction genre'] = registered['Fiction genre'].apply(clean_genres)\n",
    "registered['Nonfiction genre'] = registered['Nonfiction genre'].apply(clean_genres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's replace a few of the ones that have ' in them, which make things tricky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "registered['Fiction genre'] = registered['Fiction genre'].str.replace(\"Women’s\", \"Women's\")\n",
    "registered['Fiction genre'] = registered['Fiction genre'].str.replace(\"Children’s picture/chapter books\", \"Children's picture/chapter books\")\n",
    "\n",
    "registered['Nonfiction genre'] = registered['Nonfiction genre'].str.replace(\"Women’s issues\", \"Women's issues\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix date-times and emails\n",
    "\n",
    "We need to change the registration date to a date_time variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "registered[\"datetime\"] = pd.to_datetime(registered[\"Registration Date\"])\n",
    "waitlist[\"datetime\"] = pd.to_datetime(waitlist[\"Registration Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see if every Email Address is associated with a unique first and last name, since ideally we just use email as our unique identifier. It's possible spouses use the same email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(registered['Email'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = registered[['Email', 'First Name']].value_counts().reset_index()\n",
    "len(check['Email'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. The number of unique emails match, whether we just look at email, or if we also look at email and first name. Moving forward, we can use email address as a unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix phone numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for any phone numbers (in both the waitlist and registered files) that aren't just 10 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25     678-708-3046\n",
      "105    801-390-4595\n",
      "106    801-390-4595\n",
      "Name: Mobile Phone Number, dtype: object\n",
      "After fixing waitlist phone numbers, there are now 0 phones with dashes or parentheses\n"
     ]
    }
   ],
   "source": [
    "phonecheck_wait = waitlist.loc[waitlist['Mobile Phone Number'].str.contains(\"-\")]\n",
    "print(phonecheck_wait['Mobile Phone Number'])\n",
    "\n",
    "waitlist['phone'] = waitlist['Mobile Phone Number'].str.replace(r'^(?:\\(\\+\\d+\\))|\\D', '', regex=True)\n",
    "print(\"After fixing waitlist phone numbers, there are now\", len(waitlist.loc[waitlist['phone'].str.contains(\"-\")]), \"phones with dashes or parentheses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         404-941-0572\n",
      "4         828-279-6154\n",
      "35        404-429-4890\n",
      "95        828-279-6154\n",
      "97        801-390-4595\n",
      "             ...      \n",
      "1757      404-941-0572\n",
      "1769      201-906-3051\n",
      "1774      678-708-3046\n",
      "1791      410-746-0590\n",
      "1815    (517) 944-2233\n",
      "Name: Mobile Phone Number, Length: 116, dtype: object\n",
      "After fixing registered phone numbers, there are now 0 phones with dashes or parentheses\n"
     ]
    }
   ],
   "source": [
    "phonecheck_reg = registered.loc[registered['Mobile Phone Number'].str.contains(\"-\")]\n",
    "print(phonecheck_reg['Mobile Phone Number'])\n",
    "\n",
    "registered['phone'] = registered['Mobile Phone Number'].str.replace(r'^(?:\\(\\+\\d+\\))|\\D', '', regex=True)\n",
    "print(\"After fixing registered phone numbers, there are now\", len(registered.loc[registered['phone'].str.contains(\"-\")]), \"phones with dashes or parentheses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that all phone numbers are ten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Mobile Phone Number         phone\n",
      "41       '+49 1704174774  491704174774\n",
      "184      '+49 1704174774  491704174774\n",
      "349      '+49 1704174774  491704174774\n",
      "490      '+49 1704174774  491704174774\n",
      "648      '+49 1704174774  491704174774\n",
      "778      '+49 1704174774  491704174774\n",
      "819      '+49 1704174774  491704174774\n",
      "868      '+49 1704174774  491704174774\n",
      "946      '+49 1704174774  491704174774\n",
      "1071     '+49 1704174774  491704174774\n",
      "1146     '+49 1704174774  491704174774\n",
      "1279     '+49 1704174774  491704174774\n",
      "1292     '+49 1704174774  491704174774\n",
      "1486     '+49 1704174774  491704174774\n",
      "1645     '+49 1704174774  491704174774\n",
      "1704     '+49 1704174774  491704174774\n"
     ]
    }
   ],
   "source": [
    "phonecheck_reg = registered.loc[registered['phone'].str.len()>10]\n",
    "print(phonecheck_reg[['Mobile Phone Number', 'phone']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Mobile Phone Number, phone]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "phonecheck_wait = waitlist.loc[waitlist['phone'].str.len()>10]\n",
    "print(phonecheck_wait[['Mobile Phone Number', 'phone']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix both these datasets, so anyone with an international number gets their phone reset to missing (though we'll keep the original Mobile Phone Number column intact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "16\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "registered.loc[registered['phone'].str.len()>10, 'phone'] = None\n",
    "registered['phone'].head()\n",
    "\n",
    "waitlist.loc[waitlist['phone'].str.len()>10, 'phone'] = None\n",
    "waitlist['phone'].head()\n",
    "\n",
    "print(registered['Mobile Phone Number'].isna().sum())\n",
    "print(registered['phone'].isna().sum())\n",
    "\n",
    "print(waitlist['Mobile Phone Number'].isna().sum())\n",
    "print(waitlist['phone'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! We didn't have any missing values to begin with, but we reset those 16 international numbers to missing for the phone column, but not the Mobile Phone Number column.\n",
    "\n",
    "Let's move on to email addresses now, and check for any that are missing or problematic. First, we'll check if any are missing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "weird_emails = registered.loc[registered['Email'].isna(), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Everyone filled out an email. So now we just need to check that nobody put in faulty emails that will cause problems later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138     jlary@alumni.iu.edu\n",
      "340     jlary@alumni.iu.edu\n",
      "506     jlary@alumni.iu.edu\n",
      "611     jlary@alumni.iu.edu\n",
      "955     jlary@alumni.iu.edu\n",
      "1223    jlary@alumni.iu.edu\n",
      "1280    jlary@alumni.iu.edu\n",
      "1309    jlary@alumni.iu.edu\n",
      "1628    jlary@alumni.iu.edu\n",
      "1664    jlary@alumni.iu.edu\n",
      "Name: Email, dtype: object\n"
     ]
    }
   ],
   "source": [
    "weird_emails = registered.loc[registered['Email'].str.contains(r'^[\\w\\.-]+@[a-zA-Z\\d-]+\\.[a-zA-Z]{2,}$', regex=True)==False, ]\n",
    "print(weird_emails['Email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: Email, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "weird_emails = waitlist.loc[waitlist['Email'].str.contains(r'^[\\w\\.-]+@[a-zA-Z\\d-]+\\.[a-zA-Z]{2,}$', regex=True)==False, ]\n",
    "print(weird_emails['Email'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, the emails all look fine. That particular Alumni email isn't a problem, so emails are good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add in virtual variable to the waitlist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "waitlist['virtual'] = waitlist['Email'].apply(\n",
    "    lambda email: 'Virtual' if email in virtual_only['Email'].values else 'In person'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop any unneeded variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop any extraneous variables from the waitlist and registration datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "waitlist.drop(columns=['Registration Date', 'Invitee Status', 'Action', 'Confirmation Number'],axis=1, inplace=True) # columns are 1, rows are 0\n",
    "\n",
    "registered.drop(columns=['Agenda Item Type', 'Registration Date', 'Registration Type', 'Action'],axis=1, inplace=True) # columns are 1, rows are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(weird_emails, phonecheck_reg, phonecheck_wait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "today = datetime.date.today().strftime('%Y-%m-%d') # Let's save today's date for when writing excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bring in timekeeper information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load the time keepers\n",
    "timekeepers = pd.read_excel('List_of_genres_agents_editors.xlsx', sheet_name='timekeepers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create lists with all time-by-room values\n",
    "We need to pull in the start times for each of the time slots for Friday afternoon (query letter critiques), Saturday morning (manuscript critiques), and Saturday afternoon (pitches) sessions. Without worrying about who our timekeepers are, or which agents are assigned to those rooms, we'll create 3 lists with the times-by-room."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "room_fr = pd.read_excel('List_of_genres_agents_editors.xlsx', sheet_name='rooms_friday')\n",
    "room_sat = pd.read_excel('List_of_genres_agents_editors.xlsx', sheet_name='rooms_sat')\n",
    "timeslots = pd.read_excel('List_of_genres_agents_editors.xlsx', sheet_name='timeslots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "rooms_friday = room_fr.loc[:, 'day':'room_name']\n",
    "rooms_saturday = room_sat.loc[:, 'day':'room_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine the timeslots dataset with the friday and saturday rooms datasets to get the lists we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "tslist_fri = pd.merge(timeslots.loc[(timeslots['day']=='Friday') & (timeslots['day_session']=='Afternoon'), :], rooms_friday, how='outer', on='day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "rooms_coach = pd.read_excel('List_of_genres_agents_editors.xlsx', sheet_name='coaches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "tslist_coach = pd.merge(timeslots.loc[(timeslots['day']=='Friday') & (timeslots['day_session']=='Coaching'), :], rooms_coach, how='outer', on='day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load the agent-editor pairs for Friday and their rooms\n",
    "final_room_pairings_Friday = pd.read_excel(\"Outputs/Finalized datasets/Editor-agent pairings for Friday_2025-01-24.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Friday Query Letter Critiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign CHANGED participants timeslots/publisher pairings for Friday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we ran the code to schedule all participants. Now, however, we want to run update that so we drop anyone who is NO LONGER scheduled for that event (e.g., drop outs), and to then schedule anyone new. We'll do this for Friday first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataset with a single row per participant and all their relevant activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first identify every participant who registered for a query letter critique on Friday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "query_critique_names = registered.loc[registered['Agenda Item Name'].str.contains('Query Letter Critique'), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a count of each email in this list, so we know the number of query letter critiques each person signed up for. Then we'll delete the original query_critique_names dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "queries = query_critique_names['Email'].value_counts().reset_index()\n",
    "del(query_critique_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, for Friday's assignments, we can't assign people to agents/editors they're seeing on Saturday for a pitch or manuscript critique. In order to account for this, we also need to create datasets for the manuscript and pitches, so we can combine all three datasets later. \n",
    "\n",
    "Our goal is to create a single row per participant that lists any agents/editors they chose on Saturday, and to have know how many query letter critiques those people want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pitches = registered.loc[registered['Agenda Item Name'].str.contains('Pitch'), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract out the publisher name from the Agenda Item Name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-893>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pitches['pubname'] = pitches['Agenda Item Name'].str.replace(\"Pitch [A-Z] with \", \"\", regex=True)\n",
    "pitch = pitches[['Email', 'pubname']].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Email, pubname, count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(pitch.loc[pitch['count']>1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**NOTE:**</font> In an ideal world, there should be nobody printed above. Everyone should have a count of 1, since they can't meet with and pitch the same publisher multiple times for the same book. However, very, very rarely, someone will want to meet with a publisher twice to pitch them *different* books, so there can be counts of two or more.\n",
    "\n",
    "We always want to confirm this with the participants though, to confirm that the double booking was intentional and not a registration error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**UPDATE AFTER SPEAKING WITH GEORGE:**</font> This person (tk@tkread.com) DOES want 2 pitches with the same person. She has two different manuscripts to pitch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've checked that, please note that people can sign up for up to three pitches (typically with 3 different agents/editors). We now need to create a combined variable per registrant that has ALL their pitch agents/editors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pitchA = pitches.loc[pitches['Agenda Item Name'].str.contains(\"Pitch A with \"), ['Email', 'pubname']]\n",
    "pitchB = pitches.loc[pitches['Agenda Item Name'].str.contains(\"Pitch B with \"), ['Email', 'pubname']]\n",
    "pitchC = pitches.loc[pitches['Agenda Item Name'].str.contains(\"Pitch C with \"), ['Email', 'pubname']]\n",
    "\n",
    "pitchA = pitchA.rename(columns={'pubname':'pitchA'})\n",
    "pitchB = pitchB.rename(columns={'pubname':'pitchB'})\n",
    "pitchC = pitchC.rename(columns={'pubname':'pitchC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105 entries, 0 to 104\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Email   105 non-null    object\n",
      " 1   pitchA  73 non-null     object\n",
      " 2   pitchB  69 non-null     object\n",
      " 3   pitchC  58 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.4+ KB\n"
     ]
    }
   ],
   "source": [
    "pitch2 = pd.merge(pd.merge(pitchA, pitchB, how='outer', on='Email'), pitchC, how='outer', on='Email')\n",
    "pitch2.info()\n",
    "# Reset anybody with the same values - in our case, we don't want this to happen, since the duplicated person is intentional\n",
    "#pitch2.loc[(pitch2['pitchA'] == pitch2['pitchB']), 'pitchB'] = np.nan\n",
    "#pitch2.loc[(pitch2['pitchA'] == pitch2['pitchC']), 'pitchC'] = np.nan\n",
    "#pitch2.loc[(pitch2['pitchB'] == pitch2['pitchC']), 'pitchC'] = np.nan\n",
    "#pitch2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a combined variable of everyone's chosen publishers for their pitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def combine_variables(row):\n",
    "    return ', '.join(str(x) for x in row.dropna()) #convert to strings, drop Nas, and join.\n",
    "\n",
    "pitch2['pitches_chosen_pubs'] = pitch2[['pitchA', 'pitchB', 'pitchC']].apply(combine_variables, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's repeat this process for manuscript critiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ms = registered.loc[registered['Agenda Item Name'].str.contains('Manuscript'), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-899>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms['pubname'] = ms['Agenda Item Name'].str.replace(\"Manuscript Critique [A-Z] with \", \"\", regex=True)\n",
    "manuscript = ms[['Email', 'pubname']].value_counts().reset_index()\n",
    "len(ms['Email'].unique()) == len(manuscript['Email'].unique())\n",
    "len(ms['Email'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. Nobody signed up for duplicate manuscript critiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "msA = ms.loc[ms['Agenda Item Name'].str.contains(\"Manuscript Critique A with \"), ['Email', 'pubname']]\n",
    "msB = ms.loc[ms['Agenda Item Name'].str.contains(\"Manuscript Critique B with \"), ['Email', 'pubname']]\n",
    "msC = ms.loc[ms['Agenda Item Name'].str.contains(\"Manuscript Critique C with \"), ['Email', 'pubname']]\n",
    "\n",
    "msA = msA.rename(columns={'pubname':'msA'})\n",
    "msB = msB.rename(columns={'pubname':'msB'})\n",
    "msC = msC.rename(columns={'pubname':'msC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "manuscript = pd.merge(pd.merge(msA, msB, how='outer', on='Email'), msC, how='outer', on='Email')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "manuscript['ms_chosen_pubs'] = manuscript[['msA', 'msB', 'msC']].apply(combine_variables, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(pitch, pitches)\n",
    "queries = queries.rename(columns={'count': 'num_query_critiques'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! Okay, now it's time to merge the pitch and the manuscript info, and then link it back to the query critiques as well, so we have the full list of participants with all of their chosen editors, and whether or not they have any query letter critiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "merge1 = pd.merge(manuscript, pitch2, how='outer', on='Email')[['Email', 'pitchA', 'pitchB', 'pitchC', 'msA', 'msB', 'msC', 'pitches_chosen_pubs', 'ms_chosen_pubs']]\n",
    "email_set = set(queries['Email'].dropna())\n",
    "merge2 = pd.merge(merge1, queries, how='outer', on='Email')\n",
    "merge2['query_critique'] = merge2['Email'].apply(lambda email: email in email_set if pd.notna(email) else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(merge1, pitch2, manuscript, queries, room_fr, room_sat, email_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perrrfect. Last step is to create a dataset with one row per email, which has their fiction and non-fiction genres, as well as if they're virtual or remote. We'll then join this to our dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "per_registrant = registered.drop_duplicates(subset='Email', keep='first')[['Email', 'Virtual', 'Fiction genre', 'Nonfiction genre', 'datetime']]\n",
    "per_registrant['Virtual'] = per_registrant['Virtual'].replace(['Virtually via Zoom (only available for query letter critiques, manuscript sample critiques, and pitches)', 'In person at the conference hotel'],\n",
    "                                                              ['Virtual', 'In person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual\n",
      "In person                                                      142\n",
      "Virtual                                                         18\n",
      "Only doing the pre-conference edit (which will be by email)      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(per_registrant['Virtual'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll rename the Email Address to Email, and then we'll merge the dataframes to get one big one with all participants who registered for any of the three main activities: query letter critiques, manuscripts, or pitches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "per_registrant2 = pd.merge(per_registrant, merge2, how='outer', on='Email')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create 3 different datasets: one per query letter critiques, MS critiques, and pitches\n",
    "Before doing any scheduling, we need to create 3 different datasets for these three different activities, so we can easily schedule them below in their respective sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ms_critiques = per_registrant2.loc[pd.notna(per_registrant2['ms_chosen_pubs']), ['Email', 'Virtual', 'ms_chosen_pubs', 'msA', 'msB', 'msC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pitches = per_registrant2.loc[pd.notna(per_registrant2['pitches_chosen_pubs']), ['Email', 'Virtual', 'pitches_chosen_pubs', 'pitchA', 'pitchB', 'pitchC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "query_critiques = per_registrant2.loc[per_registrant2['query_critique']==True, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(per_registrant,merge2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, before we finalize this dataset, we also need to account for people who signed up for the Friday workshop from 4-6pm. These people need to be assigned query letter critiques prior to 4pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "fri_workshop = registered[registered['Agenda Item Name']=='Friday Workshop- Writer Beware: How Writers Can Protect Themselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-914>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "query_critiques['Friday_workshop'] = query_critiques['Email'].isin(fri_workshop['Email'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code below, we're going to bring in everyone we previously assigned to timeslots and check to see for the following 3 scenarios:\n",
    "\n",
    "1) Someone dropped out of 1-2 query letter critiques (either dropped completely, or just dropped from 2 to 1)\n",
    "2) Somebody signed up for 1-2 query letter critiques that hadn't had any before\n",
    "3) Somebody signed up for an additional query letter critique (changed from 1 to 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Bring in the prior assignments\n",
    "prior_query = pd.read_excel(\"Outputs/Finalized datasets/Expanded_query_critiques-ds prior to assignment_2025-01-24.xlsx\")\n",
    "prior_friday_queries = pd.read_excel(\"Outputs/Finalized datasets/Final_Friday_query_letter_critique_assignments_2025-01-24.xlsx\")\n",
    "\n",
    "# Get the list of unique participants doing query letters from last time, and check who this has changed for:\n",
    "prior_unique_query = prior_query.groupby('Email', as_index=False).size()\n",
    "\n",
    "# Rename the 'size' column to 'num_queries'\n",
    "prior_unique_query.rename(columns={'size': 'num_query_critiques'}, inplace=True)\n",
    "\n",
    "droppedout = prior_unique_query.merge(\n",
    "    query_critiques,\n",
    "    on=['Email', 'num_query_critiques'],\n",
    "    how='left',  # Keep all rows from `prior_unique_query`\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# Keep rows that are only in `prior_unique_query`\n",
    "droppedout = droppedout[droppedout['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "newpeeps = prior_unique_query.merge(\n",
    "    query_critiques,\n",
    "    on=['Email', 'num_query_critiques'],\n",
    "    how='right',  # Keep all rows from `prior_unique_query`\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# Keep rows that are only in `prior_unique_query`\n",
    "newpeeps = newpeeps[newpeeps['_merge'] == 'right_only'].drop(columns=['_merge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! For anyone that dropped out, let's drop them from the prior list. <font color='red'>** NOTE: YOU WILL NEED TO DOUBLE CHECK THIS- IF SOMEBODY JUST DROPPED FROM ONE, BUT STILL HAS ONE LEFT, THE BELOW CODE WILL DROP THEM FROM BOTH...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "prior_friday_queries = prior_friday_queries[~prior_friday_queries['Email'].isin(droppedout['Email'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below will adjust the query manuscripts dataset so that there's as many rows per person as there are counts for them. This way, anyone who signed up for two query critiques will have two rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def expand_dataframe(df, id_col, count_col):\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        count = int(row[count_col])  # Convert float count to integer\n",
    "        for _ in range(count):\n",
    "            rows.append(row.drop(count_col).to_dict())\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "expanded_query_critiques = expand_dataframe(newpeeps, 'Email', 'num_query_critiques')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign participants to their Friday timeslots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the prior sections of code, we created:\n",
    "\n",
    "1) The rooms for Friday with the agents/editors assigned to them (final_room_pairings_Friday)\n",
    "2) The list of all time slots and rooms for Friday (tslist_fri)\n",
    "3) The list of all participants who signed up for a query letter critique (expanded_query_critiques), which has multiple rows per person - one for the number of queries they signed up for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on though, let's join #1 and #2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "times_friday = tslist_fri.merge(final_room_pairings_Friday, on=['day', 'room_name'], how='outer').sort_values(['timeslot_start', 'room_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Bring in the final_cross_pubs dataset too\n",
    "final_cross_pubs = pd.read_excel(\"Outputs/Finalized datasets/publisher_pair_rankings_2025-01-24.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also bring in the information on what combined genres those publisher pairings represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "times_friday2 = times_friday.merge(final_cross_pubs[['pubname1', 'pubname2', 'combined_fiction', 'combined_nonfiction']], on=['pubname1', 'pubname2'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# First, we need to create lists for anything separated by a comma\n",
    "times_friday2['combined_nonfiction'] = times_friday2['combined_nonfiction'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])\n",
    "times_friday2['combined_fiction'] = times_friday2['combined_fiction'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])\n",
    "\n",
    "newpeeps['nonfiction_genre'] = newpeeps['Nonfiction genre'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])\n",
    "newpeeps['fiction_genre'] = newpeeps['Fiction genre'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])\n",
    "newpeeps['pitches_chosen_pubs'] = newpeeps['pitches_chosen_pubs'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])\n",
    "newpeeps['ms_chosen_pubs'] = newpeeps['ms_chosen_pubs'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Combine the publishers into a single list\n",
    "newpeeps['chosen_pubs'] = newpeeps['pitches_chosen_pubs'] + newpeeps['ms_chosen_pubs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the 'timeslot_start' to datetime variable\n",
    "times_friday2['timeslot_start'] = pd.to_datetime(date_str_fri + ' ' + times_friday2['timeslot_start'].astype(str))\n",
    "\n",
    "# Adjust the times to represent the afternoon (add 12 hours if in AM range)\n",
    "times_friday2['timeslot_start'] = times_friday2['timeslot_start'].apply(\n",
    "    lambda x: x + pd.Timedelta(hours=12) if x.hour < 12 else x\n",
    ")\n",
    "\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ONLY going to run the code for the newpeeps dataset. However, first we need to drop any previously assigned times and editor-agent pairs from the times_friday2, so we only have the NEW slots that need assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Keep only times that were NOT previously assigned and that still need participants assigned to them.\n",
    "\n",
    "merged = times_friday2.merge(\n",
    "    prior_friday_queries[['timeslot_start', 'room_name']],\n",
    "    on=['timeslot_start', 'room_name'],\n",
    "    how='left',  # Keep all rows from `times_friday2`\n",
    "    indicator=True  # Add an indicator column showing the source of each row\n",
    ")\n",
    "\n",
    "# Keep only rows that are not in prior_friday_queries\n",
    "new_times_friday2 = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "del(merged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For assignment purposes, we're going to prioritize people according to the following:\n",
    "1) Virtual\n",
    "2) How many publishers they signed up with for manuscript critiques and/or pitches\n",
    "3) Friday workshop attendees\n",
    "4) Registration date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Sort participants by prioritization criteria\n",
    "newpeeps['chosen_pubs_count'] = newpeeps['chosen_pubs'].apply(len)\n",
    "newpeeps.sort_values(\n",
    "    by=['Virtual', 'chosen_pubs_count', 'Friday_workshop', 'datetime'],\n",
    "    ascending=[False, False, True, True],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incompatible due to genres.\n",
      "Participant fiction genres: {\"Children's picture/chapter books\"}\n",
      "Publisher fiction genres: {\"Women's\", 'Humor', 'LGBTQ+', 'Magical realism', 'Science fiction', 'Mainstream/commercial', 'Romance', 'Southern', 'Thriller', 'Young adult', 'Historical', 'Coming-of-age', 'Horror/Supernatural', 'Fantasy', 'Contemporary', 'Speculative fiction/myths & fairy tales', 'Middle grade', 'Upmarket commercial/book club'}\n",
      "Participant nonfiction genres: {'Spiritual/inspirational/religious'}\n",
      "Publisher nonfiction genres: set()\n",
      "Participant mogboz@gmail.com not compatible with slot 2025-05-02 15:30:00 in room Board Room VI\n",
      "Assigned mogboz@gmail.com to 2025-05-02 15:30:00 in room Fayetteville\n"
     ]
    }
   ],
   "source": [
    "# Initialize an assignment dictionary\n",
    "assignments = []\n",
    "\n",
    "# Create a new dataset with all the people:\n",
    "participants_df = newpeeps.copy()\n",
    "slots_df = new_times_friday2\n",
    "\n",
    "# Helper function to check slot compatibility\n",
    "def is_slot_compatible(participant, slot):\n",
    "    # Check if at least one genre matches\n",
    "    participant_fiction_genres = set(participant['fiction_genre'])\n",
    "    slot_fiction_genres = set(slot['combined_fiction'])\n",
    "\n",
    "    participant_nonfiction_genres = set(participant['nonfiction_genre'])\n",
    "    slot_nonfiction_genres = set(slot['combined_nonfiction'])\n",
    "\n",
    "    if not (participant_fiction_genres & slot_fiction_genres or participant_nonfiction_genres & slot_nonfiction_genres):\n",
    "        print(f\"Incompatible due to genres.\") \n",
    "        print(f\"Participant fiction genres: {participant_fiction_genres}\")\n",
    "        print(f\"Publisher fiction genres: {slot_fiction_genres}\")\n",
    "        print(f\"Participant nonfiction genres: {participant_nonfiction_genres}\") \n",
    "        print(f\"Publisher nonfiction genres: {slot_nonfiction_genres}\")\n",
    "        return False\n",
    "\n",
    "    # Check publisher overlap\n",
    "    participant_pubs = set(participant['chosen_pubs'])\n",
    "    slot_pubs = {slot['pubname1'], slot['pubname2']}\n",
    "    if participant_pubs.intersection(slot_pubs):\n",
    "        print(f\"Incompatible due to publisher overlap. Participant publishers: {participant_pubs}, Slot publishers: {slot_pubs}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Iterate through slots and assign participants\n",
    "for _, slot in slots_df.iterrows():\n",
    "    if participants_df.empty:\n",
    "        break  # Exit if no participants are left to assign\n",
    "\n",
    "    for index, participant in participants_df.iterrows():\n",
    "        # Skip participants already assigned to conflicting slots\n",
    "        assigned_slots = [a['timeslot_start'] for a in assignments if a['Email'] == participant['Email']]\n",
    "        if any(abs(slot['timeslot_start'] - assigned) <= timedelta(minutes=15) for assigned in assigned_slots):\n",
    "            print(f\"Participant {participant['Email']} skipped due to conflicting time slot.\")\n",
    "            continue\n",
    "\n",
    "        # Check slot compatibility\n",
    "        if is_slot_compatible(participant, slot):\n",
    "            assignments.append({\n",
    "                'Email': participant['Email'],\n",
    "                'timeslot_start': slot['timeslot_start'],\n",
    "                'room_name': slot['room_name'],\n",
    "                'pubname1': slot['pubname1'],\n",
    "                'pubname2': slot['pubname2'],\n",
    "                'participant_fiction_genre': ', '.join(participant['fiction_genre']),\n",
    "                'participant_nonfiction_genre': ', '.join(participant['nonfiction_genre']),\n",
    "                'publisher_fiction_genre': slot['combined_fiction'],\n",
    "                'publisher_nonfiction_genre': slot['combined_nonfiction'],\n",
    "                'workshop': participant['Friday_workshop'],\n",
    "                'virtual':participant['Virtual']\n",
    "            })\n",
    "\n",
    "            print(f\"Assigned {participant['Email']} to {slot['timeslot_start']} in room {slot['room_name']}\")\n",
    "\n",
    "            # Remove the assigned participant row\n",
    "            participants_df.drop(index, inplace=True)\n",
    "            break\n",
    "        else:\n",
    "                print(f\"Participant {participant['Email']} not compatible with slot {slot['timeslot_start']} in room {slot['room_name']}\")\n",
    "\n",
    "\n",
    "# Convert assignments to a DataFrame\n",
    "assignments_df = pd.DataFrame(assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(len(participants_df))\n",
    "print(len(expanded_query_critiques)== len(assignments_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**NOTE**: THERE WILL ALMOST DEFINITELY BE SOME PEOPLE WHO COULDN'T BE ASSIGNED. YOU WILL NEED TO MANUALLY CORRECT STUFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Flag participants with a Friday workshop and slots from 4-5pm\n",
    "\n",
    "assignments_df['Flag'] = assignments_df.apply(\n",
    "    lambda x: (x['workshop']) and (x['timeslot_start'].hour >= 16),  # After 4pm check\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flag\n",
      "False    1\n",
      "True     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(assignments_df['Flag'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, awesome! We're good to go, and now let's just print out the csv file with all the assignments, and also save the dataset as a final (better named) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_friday_assignments = assignments_df\n",
    "\n",
    "del(assignments_df, participants_df, slots_df, is_slot_compatible, expand_dataframe, clean_genres, assigned_slots, assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Merge back in with the original dataset:\n",
    "all_friday_assignments = pd.merge(final_friday_assignments, prior_friday_queries, how='outer', on=['Email', 'timeslot_start', 'room_name', 'pubname1', 'pubname2', 'participant_fiction_genre', 'participant_nonfiction_genre', 'workshop', 'virtual', 'Flag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, create a column called 'publisher' that is a merging of the two publishers names, and also add a variable 'Session' that says 'Query Letter Critiques'. Oh, and add in a variable for 'Timekeeper', which is True/False depending on if the participant is a timekeeper that day or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "all_friday_assignments['publisher'] = all_friday_assignments['pubname1'] + \" and \" + all_friday_assignments['pubname2']\n",
    "all_friday_assignments['Session'] = \"Query Letter Critiques\"\n",
    "all_friday_assignments['Timekeeper'] = all_friday_assignments['Email'].isin(timekeepers['Email'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's link in the first and last names,as well as phone numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_friday_assignments2 = pd.merge(all_friday_assignments, registered[['Email', 'First Name', 'Last Name', 'phone']].drop_duplicates(), on=\"Email\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_friday_assignments2.to_excel(\"Outputs/Finalized datasets/FINAL_Friday_QLC_assignments.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**NOTE**</font> Double check if any of the new people are workshop people that were assigned after 4pm. You may need to manually change a few people's information in the excel and then reload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_friday_assignments2 = pd.read_excel(\"Outputs/Finalized datasets/FINAL_Friday_QLC_assignments.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(final_friday_assignments, droppedout, newpeeps, expanded_query_critiques, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Friday Author Coaching - <font color='red'>**FIX FROM HERE ON**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-777>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<positron-console-cell-777>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "reg_coaching = registered.loc[registered['Agenda Item Name'].str.contains(\"Coach\"), :]\n",
    "reg_coaching['Friday_workshop'] = reg_coaching['Email'].isin(fri_workshop['Email'])\n",
    "reg_coaching['QLC'] = reg_coaching['Email'].isin(final_friday_assignments2['Email'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we need to update the timeslots thing so that they are datetimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the 'timeslot_start' to datetime variable\n",
    "tslist_coach['timeslot_start'] = pd.to_datetime(date_str_fri + ' ' + tslist_coach['timeslot_start'].astype(str))\n",
    "\n",
    "# Adjust the times to represent the afternoon (add 12 hours if in AM range)\n",
    "tslist_coach['timeslot_start'] = tslist_coach['timeslot_start'].apply(\n",
    "    lambda x: x + pd.Timedelta(hours=12) if x.hour < 12 else x\n",
    ")\n",
    "\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# We need to create an alternate version of the friday assignments dataset so that we can add buffer times for our purposes here:\n",
    "publisher_meetings = final_friday_assignments2.copy()\n",
    "publisher_meetings['timeslot_end'] = publisher_meetings['timeslot_start'] + timedelta(minutes=15)\n",
    "publisher_meetings['buffer_start'] = publisher_meetings['timeslot_start'] - timedelta(minutes=15)\n",
    "publisher_meetings['buffer_end'] = publisher_meetings['timeslot_end'] + timedelta(minutes=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# We also need to add end times for the coaching meetings. Note that they're technically 15 minutes, but we'll ignore that and pretend they're 17 since there's a two minute break between them\n",
    "tslist_coach['timeslot_end'] = tslist_coach['timeslot_start'] + timedelta(minutes=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load the previous assignments\n",
    "prior_coaching = pd.read_excel(\"Outputs/Finalized datasets/Finalized coaching schedule_2025-01-24.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Identify any new people and any dropouts\n",
    "newpeeps = reg_coaching[~reg_coaching['Email'].isin(prior_coaching['Email'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "coaching_timeslots = tslist_coach.copy()\n",
    "\n",
    "# Assign coaching meetings\n",
    "def assign_coaching_meetings():\n",
    "    coaching_schedule = []\n",
    "\n",
    "    for _, participant in reg_coaching.iterrows():\n",
    "        Email = participant['Email']\n",
    "        selected_coach = participant['Agenda Item Name']\n",
    "        workshop_flag = participant['Friday_workshop']\n",
    "        \n",
    "        # Exclude slots that conflict with publisher meetings\n",
    "        valid_slots = coaching_timeslots[~coaching_timeslots.apply(\n",
    "            lambda slot: any(\n",
    "                (publisher_meetings['buffer_start'] <= slot['timeslot_start']) &\n",
    "                (slot['timeslot_start'] < publisher_meetings['buffer_end'])\n",
    "            ), axis=1\n",
    "        )]\n",
    "\n",
    "        # Exclude slots after 4:00 PM for workshop participants\n",
    "        if workshop_flag:\n",
    "            valid_slots = valid_slots[valid_slots['timeslot_start'].dt.hour < 16]\n",
    "        \n",
    "        # Assign the first valid slot\n",
    "        if not valid_slots.empty:\n",
    "            chosen_slot = valid_slots.iloc[0]\n",
    "            coaching_schedule.append({\n",
    "                'Email': Email,\n",
    "                'Session': selected_coach,\n",
    "                'timeslot_start': chosen_slot['timeslot_start'],\n",
    "                'publisher': chosen_slot['coach'],\n",
    "                'room_name': chosen_slot['room_name']\n",
    "            })\n",
    "\n",
    "            # Remove the chosen slot to prevent double-booking\n",
    "            coaching_timeslots.drop(valid_slots.index[0], inplace=True)\n",
    "    \n",
    "    return pd.DataFrame(coaching_schedule)\n",
    "\n",
    "# Generate coaching schedule\n",
    "coaching_schedule = assign_coaching_meetings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(coaching_timeslots, assign_coaching_meetings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Save this scheduling\n",
    "coaching_schedule.to_excel(f\"Outputs/Finalized datasets/Finalized coaching schedule_{today}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saturday Morning - Manuscript critique scheduling\n",
    "This is the easiest scheduling assignment. Everyone has already signed up for their critiques, so we just need to make sure:\n",
    "\n",
    "1) Nobody is scheduled back-to-back (for anyone with multiple)\n",
    "2) Timekeepers aren't first or last\n",
    "3) Virtual participants are grouped back-to-back (we will prioritize them for the first time slots per room).\n",
    "\n",
    "Ideally, we also try to ensure that there's only one substitute for each time slot, though we have plenty of substitute timekeepers. This can just be a manual check and fix later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Assign the individual publishers to their respective rooms for Saturday morning (MS) and Saturday afternoon (Pitches)\n",
    "times_sat = pd.concat([rooms_saturday.reset_index(drop=True), pubs.reset_index(drop=True)], axis=1)\n",
    "\n",
    "tslist_satmorn= pd.merge(timeslots.loc[(timeslots['day']=='Saturday') & (timeslots['day_session']=='Morning'), :], times_sat, how='outer', on='day')\n",
    "tslist_sataft= pd.merge(timeslots.loc[(timeslots['day']=='Saturday') & (timeslots['day_session']=='Afternoon'), :], times_sat, how='outer', on='day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_saturday_rooms = pd.concat([rooms_saturday.reset_index(drop=True), pubs.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a single dataset for the manuscript critiques where every person has a row for their critique (as in, a person can have up to three rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(msA, msB, msC, pitchA, pitchB, pitchC) # delete these - had originally kept for this but need virtual info\n",
    "msA = ms_critiques[['Email', 'Virtual', 'msA']]\n",
    "msB = ms_critiques[['Email', 'Virtual', 'msB']]\n",
    "msC = ms_critiques[['Email', 'Virtual', 'msC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "msA = msA.rename(columns={'msA': 'publisher'})\n",
    "msB = msB.rename(columns={'msB': 'publisher'})\n",
    "msC = msC.rename(columns={'msC': 'publisher'})\n",
    "\n",
    "ms_all = pd.merge(pd.merge(msA, msB, on=['Email', 'Virtual', 'publisher'], how=\"outer\"), msC, on=['Email', 'Virtual', 'publisher'], how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop any rows with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ms_all = ms_all.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(msA, msB, msC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we just need to convert the timeslot_start to a timestamp variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the 'timeslot_start' to datetime variable\n",
    "tslist_satmorn['timeslot_start'] = pd.to_datetime(date_str_sat + ' ' + tslist_satmorn['timeslot_start'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the 'timeslot_start' to datetime variable\n",
    "tslist_sataft['timeslot_start'] = pd.to_datetime(date_str_sat + ' ' + tslist_sataft['timeslot_start'].astype(str))\n",
    "\n",
    "# Adjust the times to represent the afternoon (add 12 hours if in AM range)\n",
    "tslist_sataft['timeslot_start'] = tslist_sataft['timeslot_start'].apply(\n",
    "    lambda x: x + pd.Timedelta(hours=12) if x.hour < 12 else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also identify the timekeepers' emails. We'll make sure not to give them the first or last time slot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "timekeeps = timekeepers[['Email']].drop_duplicates()\n",
    "timekeeps = timekeeps['Email'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew! Okay, now it's time to assign the participants for the manuscript critiques. The code below works by:\n",
    "\n",
    "1) It fills alphabetically by the publisher name, so that Alexandria Brown gets all her timeslots filled first, before moving on to the next publisher in the alphabet. **NOTE**: I have it randomly filling time slots. It's not running by earliest time to latest time.\n",
    "\n",
    "2) It prioritizes assignment of participants according to how many manuscript critique slots they still need to be assigned. This means that for the first time slot it tries filling, it'll prioritize people with 3 critiques, then 2, then 1. As it continues to iterate and participants get assigned slots, a participant who initially had 3 meetings but who was already scheduled for 2 (meaning n_remaining=1) will get less priority over participants still with 2 or three meetings needing assignment.\n",
    "\n",
    "3) I randomly shuffled the participants within their priority groups. This means that participant emails are randomly ordered in the A) three remaining group, B) two remaining and C) one remaining group. This way we don't prioritize people according to the alphabetical ordering of their emails but just do random assignments. (I had implemented this because I had noticed initially that a lot of the T-Z emails weren't being assigned as readily).\n",
    "\n",
    "<font color='red'>**BIGGEST NOTE**:</font>\n",
    "This entire code is embedded within one giant function because I'm having it run this code repeatedly using different random seeds, until it finds the seed that ensures that ALL participants get assigned time slots. Then it stops and that's the seed number that's kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying seed 1...\n",
      "Trying seed 2...\n",
      "Success! All participants assigned using seed 2.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Create copies of the datasets to use in the function, since we drop participants as we go\n",
    "slots_df = tslist_satmorn\n",
    "participants_df = ms_all.copy()\n",
    "\n",
    "# Get the earliest and latest timeslots\n",
    "earliest_time = slots_df['timeslot_start'].min()\n",
    "latest_time = slots_df['timeslot_start'].max()\n",
    "\n",
    "# Define a function to perform the assignment process\n",
    "def assign_slots_with_seed(participants_df, slots_df, seed):\n",
    "\n",
    "    # Add a column to flag timekeepers in the participants dataset\n",
    "    participants_df['is_timekeeper'] = participants_df['Email'].isin(timekeeps)\n",
    "\n",
    "    # Create the blank datasets and lists for the assignments and used-up slots\n",
    "    assignments = []\n",
    "    used_slots = set()\n",
    "\n",
    "    # Add a column to track the number of meetings each participant needs\n",
    "    participants_df['remaining_meetings'] = participants_df.groupby('Email')['Email'].transform('count')\n",
    "\n",
    "    # Repeat until all participants are assigned or no more slots remain\n",
    "    while not participants_df.empty:\n",
    "        assigned_any = False\n",
    "\n",
    "        for _, slot in slots_df.iterrows():\n",
    "            room_slot_id = (slot['timeslot_start'], slot['room_name'])\n",
    "\n",
    "            if room_slot_id in used_slots:\n",
    "                continue\n",
    "\n",
    "            if participants_df.empty:\n",
    "                break\n",
    "\n",
    "            sorted_participants = (\n",
    "                participants_df\n",
    "                .sample(frac=1, random_state=seed)  # Shuffle randomly\n",
    "                .sort_values(by='remaining_meetings', ascending=False)\n",
    "            )\n",
    "\n",
    "            for index, participant in sorted_participants.iterrows():\n",
    "\n",
    "                # Skip back-to-back assignments\n",
    "                assigned_slots = [\n",
    "                    (a['timeslot_start'], a['room_name']) for a in assignments if a['Email'] == participant['Email']\n",
    "                ]\n",
    "                if any(\n",
    "                    abs(slot['timeslot_start'] - assigned_time) <= timedelta(minutes=15)\n",
    "                    for assigned_time, _ in assigned_slots\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                # Skip the earliest and latest timeslots for timekeepers if possible\n",
    "                if participant['is_timekeeper'] and slot['timeslot_start'] in [earliest_time, latest_time]:\n",
    "                    # Check if there are other slots available for this participant\n",
    "                    has_alternative = any(\n",
    "                        set(participant['publisher']) == set(alt_slot['lit_guest_name']) and\n",
    "                        alt_slot['timeslot_start'] not in [earliest_time, latest_time] and\n",
    "                        (alt_slot['timeslot_start'], alt_slot['room_name']) not in used_slots\n",
    "                        for _, alt_slot in slots_df.iterrows()\n",
    "                    )\n",
    "                    if not has_alternative:\n",
    "                        continue\n",
    "\n",
    "                if set(participant['publisher']) == set(slot['lit_guest_name']):\n",
    "                    assignments.append({\n",
    "                        'Email': participant['Email'],\n",
    "                        'timeslot_start': slot['timeslot_start'],\n",
    "                        'room_name': slot['room_name'],\n",
    "                        'publisher': slot['lit_guest_name'],\n",
    "                        'virtual': participant['Virtual'],\n",
    "                        'Session': \"Manuscript critique\",\n",
    "                        'Timekeeper': participant['is_timekeeper']\n",
    "                    })\n",
    "                    used_slots.add(room_slot_id)\n",
    "                    participants_df.drop(index, inplace=True)\n",
    "                    participants_df['remaining_meetings'] = participants_df.groupby('Email')['Email'].transform('count')\n",
    "                    assigned_any = True\n",
    "                    break\n",
    "\n",
    "        if not assigned_any:\n",
    "            break\n",
    "\n",
    "    return assignments, participants_df\n",
    "\n",
    "# Initialize variables\n",
    "success = False\n",
    "max_attempts = 1000  # Limit the number of attempts\n",
    "seed = 0\n",
    "\n",
    "while not success and seed < max_attempts:\n",
    "    seed += 1\n",
    "    print(f\"Trying seed {seed}...\")\n",
    "    \n",
    "    # Copy the original dataframes to avoid modifying them directly\n",
    "    participants_copy = ms_all.copy()\n",
    "    slots_copy = tslist_satmorn.copy()\n",
    "\n",
    "    # Run the assignment process with the current seed\n",
    "    assignments, remaining_participants = assign_slots_with_seed(participants_copy, slots_copy, seed)\n",
    "\n",
    "    # Check if all participants were assigned\n",
    "    if remaining_participants.empty:\n",
    "        success = True\n",
    "        print(f\"Success! All participants assigned using seed {seed}.\")\n",
    "        break\n",
    "\n",
    "if success:\n",
    "    # Convert assignments to a DataFrame\n",
    "    assignments_df = pd.DataFrame(assignments)\n",
    "else:\n",
    "    print(\"Failed to assign all participants within the maximum number of attempts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! This code works beautifully!! Everyone's been assigned and now let's just do a little cleaning, then repeat the process for the Saturday afternoon pitches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_satmorn_assignments = assignments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just add in the first and last names, plus phones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_satmorn_assignments2 = pd.merge(final_satmorn_assignments, registered[['Email', 'First Name', 'Last Name', 'phone']].drop_duplicates(), on='Email', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(assignments_df, ms_all, ms_critiques, participants_copy, slots_copy, slots_df, remaining_participants,\n",
    "    assignments, earliest_time, latest_time, seed, success, assign_slots_with_seed, final_friday_assignments, final_satmorn_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#  Save the dataset\n",
    "final_satmorn_assignments2.to_excel(f\"Outputs/Finalized datasets/Final manuscript critique assignments_{today}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saturday Afternoon - Pitches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the pitch assignments now! We'll do the exact same process, except using the saturday afternoon times and the pitch dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pitchA = pitches[['Email', 'Virtual', 'pitchA']]\n",
    "pitchB = pitches[['Email', 'Virtual', 'pitchB']]\n",
    "pitchC = pitches[['Email', 'Virtual', 'pitchC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pitchA = pitchA.rename(columns={'pitchA': 'publisher'})\n",
    "pitchB = pitchB.rename(columns={'pitchB': 'publisher'})\n",
    "pitchC = pitchC.rename(columns={'pitchC': 'publisher'})\n",
    "\n",
    "pitches_all = pd.merge(pd.merge(pitchA, pitchB, on=['Email', 'Virtual', 'publisher'], how=\"outer\"), pitchC, on=['Email', 'Virtual', 'publisher'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pitches_all = pitches_all.dropna()\n",
    "del(pitchA, pitchB, pitchC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying seed 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-132>:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying seed 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-132>:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying seed 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-132>:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying seed 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-132>:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying seed 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-132>:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying seed 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-132>:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying seed 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-132>:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying seed 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-132>:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying seed 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-132>:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All participants assigned using seed 9.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Get the earliest and latest timeslots\n",
    "earliest_time = tslist_sataft['timeslot_start'].min()\n",
    "latest_time = tslist_sataft['timeslot_start'].max()\n",
    "\n",
    "# Define a function to perform the assignment process\n",
    "def assign_slots_with_seed(participants_df, slots_df, seed):\n",
    "\n",
    "    # Shuffle the timeslots within each publisher group using the seed\n",
    "    shuffled_slots = (\n",
    "        slots_df.groupby('lit_guest_name', group_keys=False)\n",
    "        .apply(lambda group: group.sample(frac=1, random_state=seed))\n",
    "    )\n",
    "\n",
    "    # Add a column to flag timekeepers in the participants dataset\n",
    "    participants_df['is_timekeeper'] = participants_df['Email'].isin(timekeeps)\n",
    "\n",
    "    # Create the blank datasets and lists for the assignments and used-up slots\n",
    "    assignments = []\n",
    "    used_slots = set()\n",
    "\n",
    "    # Add a column to track the number of meetings each participant needs\n",
    "    participants_df['remaining_meetings'] = participants_df.groupby('Email')['Email'].transform('count')\n",
    "\n",
    "    # Repeat until all participants are assigned or no more slots remain\n",
    "    while not participants_df.empty:\n",
    "        assigned_any = False\n",
    "\n",
    "        for _, slot in shuffled_slots.iterrows():\n",
    "            room_slot_id = (slot['timeslot_start'], slot['room_name'])\n",
    "\n",
    "            if room_slot_id in used_slots:\n",
    "                continue\n",
    "\n",
    "            if participants_df.empty:\n",
    "                break\n",
    "\n",
    "            sorted_participants = (\n",
    "                participants_df\n",
    "                .sample(frac=1, random_state=seed)  # Shuffle randomly\n",
    "                .sort_values(by='remaining_meetings', ascending=False)\n",
    "            )\n",
    "\n",
    "            for index, participant in sorted_participants.iterrows():\n",
    "\n",
    "                # Skip back-to-back assignments\n",
    "                assigned_slots = [\n",
    "                    (a['timeslot_start'], a['room_name']) for a in assignments if a['Email'] == participant['Email']\n",
    "                ]\n",
    "                if any(\n",
    "                    abs(slot['timeslot_start'] - assigned_time) <= timedelta(minutes=15)\n",
    "                    for assigned_time, _ in assigned_slots\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                # Skip the earliest and latest timeslots for timekeepers if possible\n",
    "                if participant['is_timekeeper'] and slot['timeslot_start'] in [earliest_time, latest_time]:\n",
    "                    # Check if there are other slots available for this participant\n",
    "                    if not any(\n",
    "                        set(participant['publisher']) == set(alt_slot['lit_guest_name']) and\n",
    "                        alt_slot['timeslot_start'] not in [earliest_time, latest_time] and\n",
    "                        (alt_slot['timeslot_start'], alt_slot['room_name']) not in used_slots\n",
    "                        for _, alt_slot in shuffled_slots.iterrows()\n",
    "                    ):\n",
    "                        print(f\"Timekeeper {participant['Email']} has no alternative slot; assigning to edge slot.\")\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                if set(participant['publisher']) == set(slot['lit_guest_name']):\n",
    "                    assignments.append({\n",
    "                        'Email': participant['Email'],\n",
    "                        'timeslot_start': slot['timeslot_start'],\n",
    "                        'room_name': slot['room_name'],\n",
    "                        'publisher': slot['lit_guest_name'],\n",
    "                        'virtual': participant['Virtual'],\n",
    "                        'Session': \"Pitch\",\n",
    "                        'Timekeeper': participant['is_timekeeper']\n",
    "                    })\n",
    "                    used_slots.add(room_slot_id)\n",
    "                    participants_df.drop(index, inplace=True)\n",
    "                    participants_df['remaining_meetings'] = participants_df.groupby('Email')['Email'].transform('count')\n",
    "                    assigned_any = True\n",
    "                    break\n",
    "\n",
    "        if not assigned_any:\n",
    "            break\n",
    "\n",
    "    return assignments, participants_df\n",
    "\n",
    "# Initialize variables\n",
    "success = False\n",
    "max_attempts = 1000  # Limit the number of attempts\n",
    "seed = 0\n",
    "\n",
    "while not success and seed < max_attempts:\n",
    "    seed += 1\n",
    "    print(f\"Trying seed {seed}...\")\n",
    "    \n",
    "    # Copy the original dataframes to avoid modifying them directly\n",
    "    participants_copy = pitches_all.copy()\n",
    "    slots_copy = tslist_sataft.copy()\n",
    "\n",
    "    # Run the assignment process with the current seed\n",
    "    assignments, remaining_participants = assign_slots_with_seed(participants_copy, slots_copy, seed)\n",
    "\n",
    "    # Check if all participants were assigned\n",
    "    if remaining_participants.empty:\n",
    "        success = True\n",
    "        print(f\"Success! All participants assigned using seed {seed}.\")\n",
    "        break\n",
    "\n",
    "if success:\n",
    "    # Convert assignments to a DataFrame\n",
    "    assignments_df = pd.DataFrame(assignments)\n",
    "else:\n",
    "    print(\"Failed to assign all participants within the maximum number of attempts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! That worked great too. Let's just save it and delete any extraneous datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_sataft_assignment = assignments_df\n",
    "final_sataft_assignments2 = pd.merge(final_sataft_assignment, registered[['Email', 'First Name', 'Last Name', 'phone']].drop_duplicates(), on='Email', how='inner')\n",
    "\n",
    "del(pitches, pitches_all, assignments_df, remaining_participants, timeslots, tslist_sataft, tslist_satmorn, times_friday2, times_sat,\n",
    "    slots_copy, assignments, earliest_time, latest_time, max_attempts, success, seed, assign_slots_with_seed, timedelta, participants_copy, final_sataft_assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! Now we're officially all done with the assignments, and we just need to deal withe waitlists now. FInal step after that will be to print out everything we've got into exactly the excel and word files we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "final_sataft_assignments2.to_excel(f\"Outputs/Finalized datasets/Finalized pitch assignments_{today}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deal with the Waitlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with the waitlists is pretty simple. We already corrected some of the basic stuff earlier, like emails and phones. Now let's split into what they're waitlisted for:\n",
    "1) manuscript critiques\n",
    "2) pitches\n",
    "3) pre-conference edits\n",
    "4) book fairs\n",
    "5) query letter critiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "wait_ms = waitlist[waitlist['Session Name'].str.contains('Manuscript')]\n",
    "wait_pitch = waitlist[waitlist['Session Name'].str.contains('Pitch')]\n",
    "wait_prec = waitlist[waitlist['Session Name'].str.contains('Pre-conference')]\n",
    "\n",
    "# May also need to do bookfair and query letter critique waitlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's change all the code so that instead of Manuscript A, B, C etc, it says 'Waitlisted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-136>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<positron-console-cell-136>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "wait_pitch['Session Name'] = wait_pitch['Session Name'].str.replace(\"Pitch [A-Z] with \", \"Waitlisted - Pitch with \", regex=True)\n",
    "wait_ms['Session Name'] = wait_ms['Session Name'].str.replace(\"Manuscript Critique [A-Z] with \", \"Waitlisted - Manuscript Critique with \", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to double check that no participant has more than 3 manuscript critique waitlist spots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 1]\n",
      "[3 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(wait_pitch['Email'].value_counts().unique())\n",
    "print(wait_ms['Email'].value_counts().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. As you can see above, nobody's got 4 or higher for how often their emails appear in these lists. Now let's sort by registration date for each Session Name, so that we assign a value of #1, #2, etc. by registration date for each Manuscript critique/pitch spot with each publisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-138>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Sort by 'Session Name' and 'datetime', and rank participants\n",
    "wait_ms['Waitlist_ms'] = wait_ms.sort_values(['Session Name', 'datetime']) \\\n",
    "               .groupby('Session Name')['datetime'] \\\n",
    "               .rank(method='first').astype(int)\n",
    "\n",
    "# Sort DataFrame for display (optional)\n",
    "wait_ms = wait_ms.sort_values(['Session Name', 'Waitlist_ms']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-139>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Sort by 'Session Name' and 'datetime', and rank participants\n",
    "wait_pitch['Waitlist_pitch'] = wait_pitch.sort_values(['Session Name', 'datetime']) \\\n",
    "               .groupby('Session Name')['datetime'] \\\n",
    "               .rank(method='first').astype(int)\n",
    "\n",
    "# Sort DataFrame for display (optional)\n",
    "wait_pitch = wait_pitch.sort_values(['Session Name', 'Waitlist_pitch']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, looks good. Now let's tweak it a little bit more so we create an 'Agenda Item Name' that is 'Waitlisted - #1 - Manuscript Critique with [publisher].'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "wait_pitch['Agenda Item Name'] = wait_pitch.apply(\n",
    "    lambda row: row['Session Name'].replace(\n",
    "        \"Waitlisted\", f\"Waitlist #{row['Waitlist_pitch']}\"\n",
    "    ) if \"Waitlisted\" in row['Session Name'] else row['Session Name'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "wait_ms['Agenda Item Name'] = wait_ms.apply(\n",
    "    lambda row: row['Session Name'].replace(\n",
    "        \"Waitlisted\", f\"Waitlist #{row['Waitlist_ms']}\"\n",
    "    ) if \"Waitlisted\" in row['Session Name'] else row['Session Name'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# For right now, let's just merge all the waitlist stuff back together, and add in the participant info so that it's all in one place.\n",
    "wait_all = pd.merge(wait_ms, wait_pitch, how=\"outer\")\n",
    "\n",
    "# Let's extract the publisher\n",
    "wait_all['publisher'] = wait_all['Session Name'].str.replace(\"Waitlisted - Manuscript Critique with \", \"\")\n",
    "wait_all['publisher'] = wait_all['publisher'].str.replace(\"Waitlisted - Pitch with \", \"\")\n",
    "wait_all = wait_all[['Email', 'First Name', 'Last Name', 'phone', 'Agenda Item Name', 'publisher']]\n",
    "\n",
    "# print for George\n",
    "wait_all.to_excel(\"Outputs/Finalized Datasets/Waitlist participants.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Print a bunch of excel documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't really do much with these particular excel documents, except to export them for manual review (and potentially manual changes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_room_pairings_Friday.to_excel(f\"Outputs/Finalized Datasets/Editor-agent pairings for Friday_{today}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_friday_assignments2.to_excel(f\"Outputs/Finalized Datasets/Friday query letter critique assignments_{today}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_sataft_assignments2.to_excel(f\"Outputs/Finalized Datasets/Saturday pitch assignments_{today}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_satmorn_assignments2.to_excel(f\"Outputs/Finalized Datasets/Saturday manuscript critique assignments_{today}.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
