{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduling Participants for the May 2025 Atlanta Writer's Conference\n",
    "*Becky Hodge*\n",
    "\n",
    "#### Summary\n",
    "This code should be run ~1 month prior to the conference, and will need to be run repeatedly in the weeks and days prior to accomodate any changes.\n",
    "\n",
    "The code in this notebook imports the full list of registered and waitlist participants for the May 2025 conference, loads the (manually created) list of fiction and non-fiction genres, as well as the list of agents and timekeepers, and makes any corrections needed before moving on to the next section of code, which involves scheduling Friday's query letter critique sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**NOTE:**</font> Prior to running this code, make sure you update the *timekeepers* sheet in the *List_of_agents_editors.xlsx* document. If the assigments of the timekeepers is still TBD, that's okay - what's more important is making sure at minimum all the emails for everyone who's going to timekeeper is listed in there. That's the only part needed for this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Install any needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os \n",
    "import re\n",
    "from datetime import timedelta\n",
    "\n",
    "today = datetime.datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Set the conference dates\n",
    "date_str_fri = '2025-05-02'\n",
    "date_str_sat = '2025-05-03'\n",
    "\n",
    "# Reference the current conference folder we should be pulling and storing all datasets/excel files/templates\n",
    "current_conference_folder= \"May2025\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and clean the different files/reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Select the file with the most recent date\n",
    "directory = f'{current_conference_folder}/Cvent_report_downloads'\n",
    "\n",
    "most_recent_file = max(\n",
    "    (f for f in os.listdir(directory) if f.startswith('Registered_') and f.endswith('.csv')),\n",
    "    key=lambda x: datetime.datetime.strptime(x.split('_')[1].split('.')[0], '%m-%d-%y'),\n",
    ")\n",
    "\n",
    "# Load the most recent file\n",
    "most_recent_path = os.path.join(directory, most_recent_file)\n",
    "registered = pd.read_csv(most_recent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "most_recent_file = max(\n",
    "    (f for f in os.listdir(directory) if f.startswith('Waitlists_') and f.endswith('.csv')),\n",
    "    key=lambda x: datetime.datetime.strptime(x.split('_')[1].split('.')[0], '%m-%d-%y'),\n",
    ")\n",
    "\n",
    "# Load the most recent file\n",
    "most_recent_path = os.path.join(directory, most_recent_file)\n",
    "waitlist = pd.read_csv(most_recent_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code brings in ALL participants, which is key for knowing whether any waitlist only people are virtual or in person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "most_recent_file = max(\n",
    "    (f for f in os.listdir(directory) if f.startswith('Allparticipants_') and f.endswith('.csv')),\n",
    "    key=lambda x: datetime.datetime.strptime(x.split('_')[1].split('.')[0], '%m-%d-%y'),\n",
    ")\n",
    "\n",
    "# Load the most recent file\n",
    "most_recent_path = os.path.join(directory, most_recent_file)\n",
    "all_participants = pd.read_csv(most_recent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "all_participants = all_participants.rename(columns={'Email Address':'Email'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Filter this dataset to just virtual people\n",
    "virtual_only = all_participants.loc[all_participants['Hotel vs. Zoom'] == 'Virtually via Zoom (only available for query letter critiques, manuscript sample critiques, and pitches)', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(directory, most_recent_file, most_recent_path)\n",
    "\n",
    "fict_gen = pd.read_excel(f'{current_conference_folder}/List_of_genres_agents_editors.xlsx', sheet_name='fiction')\n",
    "nonfict_gen = pd.read_excel(f'{current_conference_folder}/List_of_genres_agents_editors.xlsx', sheet_name='nonfiction')\n",
    "pubs = pd.read_excel(f'{current_conference_folder}/List_of_genres_agents_editors.xlsx', sheet_name='agents_editors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh gosh, some of the column names are hefty...  Let's fix those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "registered = registered.rename(columns={'Hotel vs. Zoom':'Virtual', \n",
    "                                        \"What fiction genre(s) will you be presenting to agents/editors at the conference? (If you're not signing up for any agent/editor meetings, indicate which genre(s) you write.)\":'Fiction genre', \n",
    "                                        \"What nonfiction topic(s) will you be presenting to agents/editors at the conference? (If you're not signing up for any agent/editor meetings, indicate which topic(s) you write.)\":'Nonfiction genre', \n",
    "                                        'Registration Date (GMT)':'Registration Date',\n",
    "                                        'Email Address':'Email'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "waitlist = waitlist.rename(columns={'Registration Date (GMT)':'Registration Date',\n",
    "                                     'Email Address':'Email'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also fix so that we drop the 'Not applicable --I don't write fiction' and 'Not applicable--I don't write nonfiction'. We'll set them to missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "registered['Fiction genre']= registered['Fiction genre'].replace(\"Not Applicable --I don't write fiction\", np.nan)\n",
    "registered['Nonfiction genre']= registered['Nonfiction genre'].replace(\"Not Applicable--I don't write nonfiction\", np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, there's people who wrote in 'Other',  but for our purposes, we don't care about that info for the purposes of matching to agents/editors. Let's remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "def clean_genres(genre_string):\n",
    "    if genre_string is None or pd.isna(genre_string) or \"\":\n",
    "        return \"\"\n",
    "\n",
    "    genres = [genre.strip() for genre in genre_string.split(',')]\n",
    "    cleaned_genres = [genre for genre in genres if not re.match(r\"^Other \\(please specify\\):\", genre)]\n",
    "\n",
    "    return \", \".join(cleaned_genres)\n",
    "\n",
    "registered['Fiction genre'] = registered['Fiction genre'].apply(clean_genres)\n",
    "registered['Nonfiction genre'] = registered['Nonfiction genre'].apply(clean_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's replace a few of the ones that have ' in them, which make things tricky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "registered['Fiction genre'] = registered['Fiction genre'].str.replace(\"Women’s\", \"Women's\")\n",
    "registered['Fiction genre'] = registered['Fiction genre'].str.replace(\"Children’s picture/chapter books\", \"Children's picture/chapter books\")\n",
    "\n",
    "registered['Nonfiction genre'] = registered['Nonfiction genre'].str.replace(\"Women’s issues\", \"Women's issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix date-times and emails\n",
    "\n",
    "We need to change the registration date to a date_time variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "registered[\"datetime\"] = pd.to_datetime(registered[\"Registration Date\"])\n",
    "waitlist[\"datetime\"] = pd.to_datetime(waitlist[\"Registration Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see if every Email Address is associated with a unique first and last name, since ideally we just use email as our unique identifier. It's possible spouses use the same email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(registered['Email'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = registered[['Email', 'First Name']].value_counts().reset_index()\n",
    "len(check['Email'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. The number of unique emails match, whether we just look at email, or if we also look at email and first name. Moving forward, we can use email address as a unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix phone numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for any phone numbers (in both the waitlist and registered files) that aren't just 10 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104    704-965-8148\n",
      "105    704-965-8148\n",
      "106    704-965-8148\n",
      "Name: Mobile Phone Number, dtype: object\n",
      "After fixing waitlist phone numbers, there are now 0 phones with dashes or parentheses\n"
     ]
    }
   ],
   "source": [
    "phonecheck_wait = waitlist.loc[waitlist['Mobile Phone Number'].str.contains(\"-\")]\n",
    "print(phonecheck_wait['Mobile Phone Number'])\n",
    "\n",
    "waitlist['phone'] = waitlist['Mobile Phone Number'].str.replace(r'^(?:\\(\\+\\d+\\))|\\D', '', regex=True)\n",
    "print(\"After fixing waitlist phone numbers, there are now\", len(waitlist.loc[waitlist['phone'].str.contains(\"-\")]), \"phones with dashes or parentheses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175     704-965-8148\n",
      "185     217-637-3230\n",
      "212     770-655-8952\n",
      "560     704-965-8148\n",
      "576     770-655-8952\n",
      "610     217-637-3230\n",
      "626     217-637-3230\n",
      "642     770-655-8952\n",
      "659     704-965-8148\n",
      "910     217-637-3230\n",
      "934     770-655-8952\n",
      "947     704-965-8148\n",
      "1087    217-637-3230\n",
      "1183    770-655-8952\n",
      "1184    217-637-3230\n",
      "1860    770-655-8952\n",
      "1877    217-637-3230\n",
      "1879    704-965-8148\n",
      "1902    217-637-3230\n",
      "1913    770-655-8952\n",
      "2067    704-965-8148\n",
      "2079    770-655-8952\n",
      "2087    217-637-3230\n",
      "2382    217-637-3230\n",
      "2397    770-655-8952\n",
      "2415    704-965-8148\n",
      "Name: Mobile Phone Number, dtype: object\n",
      "After fixing registered phone numbers, there are now 0 phones with dashes or parentheses\n"
     ]
    }
   ],
   "source": [
    "phonecheck_reg = registered.loc[registered['Mobile Phone Number'].str.contains(\"-\")]\n",
    "print(phonecheck_reg['Mobile Phone Number'])\n",
    "\n",
    "registered['phone'] = registered['Mobile Phone Number'].str.replace(r'^(?:\\(\\+\\d+\\))|\\D', '', regex=True)\n",
    "print(\"After fixing registered phone numbers, there are now\", len(registered.loc[registered['phone'].str.contains(\"-\")]), \"phones with dashes or parentheses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that all phone numbers are ten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Mobile Phone Number         phone\n",
      "17       '+49 1704174774  491704174774\n",
      "260      '+49 1704174774  491704174774\n",
      "481      '+49 1704174774  491704174774\n",
      "783      '+49 1704174774  491704174774\n",
      "1014     '+49 1704174774  491704174774\n",
      "1230     '+49 1704174774  491704174774\n",
      "1303     '+49 1704174774  491704174774\n",
      "1437     '+49 1704174774  491704174774\n",
      "1512     '+49 1704174774  491704174774\n",
      "1662     '+49 1704174774  491704174774\n",
      "1680     '+49 1704174774  491704174774\n",
      "2018     '+49 1704174774  491704174774\n",
      "2262     '+49 1704174774  491704174774\n"
     ]
    }
   ],
   "source": [
    "phonecheck_reg = registered.loc[registered['phone'].str.len()>10]\n",
    "print(phonecheck_reg[['Mobile Phone Number', 'phone']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Mobile Phone Number, phone]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "phonecheck_wait = waitlist.loc[waitlist['phone'].str.len()>10]\n",
    "print(phonecheck_wait[['Mobile Phone Number', 'phone']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix both these datasets, so anyone with an international number gets their phone reset to missing (though we'll keep the original Mobile Phone Number column intact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "13\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "registered.loc[registered['phone'].str.len()>10, 'phone'] = None\n",
    "registered['phone'].head()\n",
    "\n",
    "waitlist.loc[waitlist['phone'].str.len()>10, 'phone'] = None\n",
    "waitlist['phone'].head()\n",
    "\n",
    "print(registered['Mobile Phone Number'].isna().sum())\n",
    "print(registered['phone'].isna().sum())\n",
    "\n",
    "print(waitlist['Mobile Phone Number'].isna().sum())\n",
    "print(waitlist['phone'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! We didn't have any missing values to begin with, but we reset those 13 international numbers to missing for the phone column, but not the Mobile Phone Number column.\n",
    "\n",
    "Let's move on to email addresses now, and check for any that are missing or problematic. First, we'll check if any are missing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "weird_emails = registered.loc[registered['Email'].isna(), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Everyone filled out an email. So now we just need to check that nobody put in faulty emails that will cause problems later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201     aflo_1@yahoo.co.uk\n",
      "271    jlary@alumni.iu.edu\n",
      "Name: Email, dtype: object\n"
     ]
    }
   ],
   "source": [
    "weird_emails = registered.loc[registered['Email'].str.contains(r'^[\\w\\.-]+@[a-zA-Z\\d-]+\\.[a-zA-Z]{2,}$', regex=True)==False, ]\n",
    "print(weird_emails['Email'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10    aflo_1@yahoo.co.uk\n",
      "Name: Email, dtype: object\n"
     ]
    }
   ],
   "source": [
    "weird_emails = waitlist.loc[waitlist['Email'].str.contains(r'^[\\w\\.-]+@[a-zA-Z\\d-]+\\.[a-zA-Z]{2,}$', regex=True)==False, ]\n",
    "print(weird_emails['Email'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, the emails all look fine. They are valid email addresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add in virtual variable to the waitlist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "waitlist['virtual'] = waitlist['Email'].apply(\n",
    "    lambda email: 'Virtual' if email in virtual_only['Email'].values else 'In person'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop any unneeded variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop any extraneous variables from the waitlist and registration datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "waitlist.drop(columns=['Registration Date', 'Invitee Status', 'Action', 'Confirmation Number'],axis=1, inplace=True) # columns are 1, rows are 0\n",
    "registered.drop(columns=['Agenda Item Type', 'Registration Date', 'Registration Type', 'Action'],axis=1, inplace=True) # columns are 1, rows are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(weird_emails, phonecheck_reg, phonecheck_wait)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bring in timekeeper information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load the time keepers\n",
    "timekeepers = pd.read_excel(f'{current_conference_folder}/List_of_genres_agents_editors.xlsx', sheet_name='timekeepers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create lists with all time-by-room values\n",
    "We need to pull in the start times for each of the time slots for Friday afternoon (query letter critiques), Saturday morning (manuscript critiques), and Saturday afternoon (pitches) sessions. Without worrying about who our timekeepers are, or which agents are assigned to those rooms, we'll create 3 lists with the times-by-room."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "room_fr = pd.read_excel(f'{current_conference_folder}/List_of_genres_agents_editors.xlsx', sheet_name='rooms_friday')\n",
    "room_sat = pd.read_excel(f'{current_conference_folder}/List_of_genres_agents_editors.xlsx', sheet_name='rooms_sat')\n",
    "timeslots = pd.read_excel(f'{current_conference_folder}/List_of_genres_agents_editors.xlsx', sheet_name='timeslots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "rooms_friday = room_fr.loc[:, 'day':'room_name']\n",
    "rooms_saturday = room_sat.loc[:, 'day':'room_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine the timeslots dataset with the friday and saturday rooms datasets to get the lists we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "tslist_fri = pd.merge(timeslots.loc[(timeslots['day']=='Friday') & (timeslots['day_session']=='Afternoon'), :], rooms_friday, how='outer', on='day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "rooms_coach = pd.read_excel(f'{current_conference_folder}/List_of_genres_agents_editors.xlsx', sheet_name='coaches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "tslist_coach = pd.merge(timeslots.loc[(timeslots['day']=='Friday') & (timeslots['day_session']=='Coaching'), :], rooms_coach, how='outer', on='day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the info on which agents and editors were paired together and are in which rooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_room_pairings_Friday = pd.read_excel(f'{current_conference_folder}/Outputs/Finalized datasets/Editor-agent pairings for Friday.xlsx')\n",
    "final_cross_pubs = pd.read_excel(f'{current_conference_folder}/Outputs/Finalized datasets/Final editor-agent pairings with combined genres.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bring in Requests and Prior Assignments and check for any changes/updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assign participants to editor-agent pairs for the conference, we need to ensure that they write in fiction and/or nonfiction genre(s) that either editor and/or agent in the pair represents. To do that, let's first identify every participant who registered for a query letter critique on Friday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "query_critique_names = registered.loc[registered['Agenda Item Name'].str.contains('Query Letter Critique'), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a count of each email in this list, so we know the number of query letter critiques each person signed up for. Then we'll delete the original query_critique_names dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "queries = query_critique_names['Email'].value_counts().reset_index()\n",
    "del(query_critique_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, for Friday's assignments, we can't assign people to agents/editors they're seeing on Saturday for a pitch or manuscript critique. In order to account for this, we also need to create datasets for the manuscript and pitches, so we can combine all three datasets later. \n",
    "\n",
    "Our goal is to create a single row per participant that lists any agents/editors they chose on Saturday, and to have know how many query letter critiques those people want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pitches = registered.loc[registered['Agenda Item Name'].str.contains('Pitch'), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract out the publisher name from the Agenda Item Name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-566>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "pitches['pubname'] = pitches['Agenda Item Name'].str.replace(\"Pitch [A-Z] with \", \"\", regex=True)\n",
    "pitch = pitches[['Email', 'pubname']].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Email, pubname, count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(pitch.loc[pitch['count']>1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**NOTE FOR THE ABOVE:**</font> In an ideal world, there should be nobody printed above. Everyone should have a count of 1, since they can't meet with and pitch the same publisher multiple times for the same book. However, very, very rarely, someone will want to meet with a publisher twice to pitch them *different* books, so there can be counts of two or more.\n",
    "\n",
    "We always want to confirm this with the participants though, to confirm that the double booking was intentional and not a registration error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've checked that, please note that people can sign up for up to three pitches (typically with 3 different agents/editors). We now need to create a combined variable per registrant that has ALL their pitch agents/editors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pitchA = pitches.loc[pitches['Agenda Item Name'].str.contains(\"Pitch A with \"), ['Email', 'pubname']]\n",
    "pitchB = pitches.loc[pitches['Agenda Item Name'].str.contains(\"Pitch B with \"), ['Email', 'pubname']]\n",
    "pitchC = pitches.loc[pitches['Agenda Item Name'].str.contains(\"Pitch C with \"), ['Email', 'pubname']]\n",
    "\n",
    "pitchA = pitchA.rename(columns={'pubname':'pitchA'})\n",
    "pitchB = pitchB.rename(columns={'pubname':'pitchB'})\n",
    "pitchC = pitchC.rename(columns={'pubname':'pitchC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 118 entries, 0 to 117\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Email   118 non-null    object\n",
      " 1   pitchA  75 non-null     object\n",
      " 2   pitchB  80 non-null     object\n",
      " 3   pitchC  69 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.8+ KB\n"
     ]
    }
   ],
   "source": [
    "pitch2 = pd.merge(pd.merge(pitchA, pitchB, how='outer', on='Email'), pitchC, how='outer', on='Email')\n",
    "pitch2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Email, pitchA, pitchB, pitchC]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Did any participants request the same person to pitch more than once?\n",
    "print(pitch2.loc[(pitch2['pitchA'] == pitch2['pitchB']) | (pitch2['pitchA'] == pitch2['pitchC']) | (pitch2['pitchB'] == pitch2['pitchC'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a combined variable of everyone's chosen publishers for their pitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def combine_variables(row):\n",
    "    return ', '.join(str(x) for x in row.dropna()) #convert to strings, drop Nas, and join.\n",
    "\n",
    "pitch2['pitches_chosen_pubs'] = pitch2[['pitchA', 'pitchB', 'pitchC']].apply(combine_variables, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's repeat this process for manuscript critiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ms = registered.loc[registered['Agenda Item Name'].str.contains('Manuscript'), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-573>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms['pubname'] = ms['Agenda Item Name'].str.replace(\"Manuscript Critique [A-Z] with \", \"\", regex=True)\n",
    "manuscript = ms[['Email', 'pubname']].value_counts().reset_index()\n",
    "len(ms['Email'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ms['Email'].unique()) == len(manuscript['Email'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. Nobody signed up for duplicate manuscript critiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "msA = ms.loc[ms['Agenda Item Name'].str.contains(\"Manuscript Critique A with \"), ['Email', 'pubname']]\n",
    "msB = ms.loc[ms['Agenda Item Name'].str.contains(\"Manuscript Critique B with \"), ['Email', 'pubname']]\n",
    "msC = ms.loc[ms['Agenda Item Name'].str.contains(\"Manuscript Critique C with \"), ['Email', 'pubname']]\n",
    "\n",
    "msA = msA.rename(columns={'pubname':'msA'})\n",
    "msB = msB.rename(columns={'pubname':'msB'})\n",
    "msC = msC.rename(columns={'pubname':'msC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "manuscript = pd.merge(pd.merge(msA, msB, how='outer', on='Email'), msC, how='outer', on='Email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "manuscript['ms_chosen_pubs'] = manuscript[['msA', 'msB', 'msC']].apply(combine_variables, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(pitch, pitches)\n",
    "queries = queries.rename(columns={'count': 'num_query_critiques'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a dataset with everyone doing the Friday workshop, so we can easily reference them later\n",
    "fri_workshop = registered[registered['Agenda Item Name'].str.contains('Friday Workshop')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting for May 2025, George added in 'Author coaching' as one of the types of sessions participates could select when registering. These are scheduled AROUND the query letter critiques (if relevant), and so as not to coincide with the Friday workshop at 4pm (if relevant).\n",
    "\n",
    "Let's make a dataset with the coaching info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "reg_coaching = registered.loc[registered['Agenda Item Name'].str.contains(\"Coach\"), :].copy()\n",
    "reg_coaching['Friday_workshop'] = reg_coaching['Email'].isin(fri_workshop['Email'])\n",
    "reg_coaching['QLC'] = reg_coaching['Email'].isin(queries['Email'])\n",
    "\n",
    "# Extract out just the coaches' names\n",
    "reg_coaching['Coach'] = reg_coaching['Agenda Item Name'].str.removeprefix(\"Author Coaching with \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! Okay, now it's time to merge the pitch and the manuscript info, and then link it back to the query critiques as well, so we have the full list of participants with all of their chosen editors, and whether or not they have any query letter critiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "merge1 = pd.merge(manuscript, pitch2, how='outer', on='Email')[['Email', 'pitchA', 'pitchB', 'pitchC', 'msA', 'msB', 'msC', 'pitches_chosen_pubs', 'ms_chosen_pubs']]\n",
    "email_set = set(queries['Email'].dropna())\n",
    "merge2 = pd.merge(merge1, queries, how='outer', on='Email')\n",
    "merge2['query_critique'] = merge2['Email'].apply(lambda email: email in email_set if pd.notna(email) else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(merge1, pitch2, manuscript, queries, room_fr, room_sat, email_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perrrfect. Last step is to create a dataset with one row per email, which has their fiction and non-fiction genres, as well as if they're virtual or remote. We'll then join this to our dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "per_registrant = registered.drop_duplicates(subset='Email', keep='first')[['Email', 'Virtual', 'Fiction genre', 'Nonfiction genre', 'datetime']]\n",
    "per_registrant['Virtual'] = per_registrant['Virtual'].replace(['Virtually via Zoom (only available for query letter critiques, manuscript sample critiques, and pitches)', 'In person at the conference hotel'],\n",
    "                                                              ['Virtual', 'In person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual\n",
      "In person    187\n",
      "Virtual       32\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(per_registrant['Virtual'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll rename the Email Address to Email, and then we'll merge the dataframes to get one big one with all participants who registered for any of the three main activities: query letter critiques, manuscripts, or pitches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "per_registrant2 = pd.merge(per_registrant, merge2, how='outer', on='Email')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 3 different datasets: one per query letter critiques, MS critiques, and pitches\n",
    "Before doing any scheduling, we need to create 3 different datasets for these three different activities, so we can easily schedule them below in their respective sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ms_critiques = per_registrant2.loc[pd.notna(per_registrant2['ms_chosen_pubs']), ['Email', 'Virtual', 'ms_chosen_pubs', 'msA', 'msB', 'msC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pitches = per_registrant2.loc[pd.notna(per_registrant2['pitches_chosen_pubs']), ['Email', 'Virtual', 'pitches_chosen_pubs', 'pitchA', 'pitchB', 'pitchC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "query_critiques = per_registrant2.loc[per_registrant2['query_critique']==True, :].copy()\n",
    "query_critiques['Friday_workshop'] = query_critiques['Email'].isin(fri_workshop['Email']) # Add in a flag for Friday workshop people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(per_registrant, merge2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring in hardcoded and prior scheduling assignments\n",
    "\n",
    "Some participants requested specific publishers and/or time slots, and these participants were 'hardcoded', with their requests being automatically handled by the code. Additionally, we want our script to basically re-assign everybody who had an assignment already to their prior spots.\n",
    "\n",
    "The one catch, however, is we also need to ensure that we catch any changes in registration - meaning, is every hardcoded and prior assigned participant still signed up for their particular activity they've been assigned to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hardcoded people\n",
    "\n",
    "Let's start by loading the hardcoded participants. This is a manually maintained excel file, and please note that for the Friday QLCs, the pubname1 and pubname2 MUST match the editor-agent pairings, and that it's using email to identify a participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "hardcoded = pd.read_excel(f'{current_conference_folder}/People to hardcode.xlsx')\n",
    "\n",
    "# Filter this to create separate QLC, MS and Pitch datasets\n",
    "hardcoded_qlc = hardcoded.loc[hardcoded['Session']=='QLC',['Email', 'timeslot_start', 'pubname1', 'pubname2']]\n",
    "hardcoded_ms = hardcoded.loc[hardcoded['Session']=='Manuscript critique', ['Email', 'timeslot_start', 'pubname1']]\n",
    "hardcoded_pitch = hardcoded.loc[hardcoded['Session']=='Pitch', ['Email', 'timeslot_start', 'pubname1']]\n",
    "hardcoded_coach = hardcoded.loc[hardcoded['Session']=='Coach', ['Email', 'timeslot_start', 'pubname1']] # This has zero rows\n",
    "\n",
    "hardcoded_ms = hardcoded_ms.rename(columns={'pubname1' : 'publisher'})\n",
    "hardcoded_pitch = hardcoded_pitch.rename(columns={'pubname1' : 'publisher'})\n",
    "hardcoded_coach = hardcoded_coach.rename(columns={'pubname1' : 'publisher'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prior assignments\n",
    "\n",
    "Now let's check for any prior assignments for the different activities: QLC, coaching, pitch, and MS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# QLC prior assignments \n",
    "\n",
    "folder = f'{current_conference_folder}/Outputs/Finalized datasets'\n",
    "matching_files = [\n",
    "    f for f in os.listdir(folder)\n",
    "    if f.startswith('Final Friday query letter critique assignments') and f.endswith('.xlsx')\n",
    "]\n",
    "\n",
    "if matching_files:\n",
    "    most_recent_file = max(\n",
    "        matching_files,\n",
    "        key=lambda x: datetime.datetime.strptime(x.split('_')[1].split('.')[0], '%Y-%m-%d')\n",
    "    )\n",
    "    most_recent_path = os.path.join(folder, most_recent_file)\n",
    "    qlc_prior_assignments = pd.read_excel(most_recent_path)\n",
    "else:\n",
    "    qlc_prior_assignments = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# MS prior assignments \n",
    "\n",
    "matching_files = [\n",
    "    f for f in os.listdir(folder)\n",
    "    if f.startswith('Final manuscript critique assignments') and f.endswith('.xlsx')\n",
    "]\n",
    "\n",
    "if matching_files:\n",
    "    most_recent_file = max(\n",
    "        matching_files,\n",
    "        key=lambda x: datetime.datetime.strptime(x.split('_')[1].split('.')[0], '%Y-%m-%d')\n",
    "    )\n",
    "    most_recent_path = os.path.join(folder, most_recent_file)\n",
    "    ms_prior_assignments = pd.read_excel(most_recent_path)\n",
    "else:\n",
    "    ms_prior_assignments = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Pitch prior assignments \n",
    "\n",
    "matching_files = [\n",
    "    f for f in os.listdir(folder)\n",
    "    if f.startswith('Finalized pitch assignments') and f.endswith('.xlsx')\n",
    "]\n",
    "\n",
    "if matching_files:\n",
    "    most_recent_file = max(\n",
    "        matching_files,\n",
    "        key=lambda x: datetime.datetime.strptime(x.split('_')[1].split('.')[0], '%Y-%m-%d')\n",
    "    )\n",
    "    most_recent_path = os.path.join(folder, most_recent_file)\n",
    "    pitch_prior_assignments = pd.read_excel(most_recent_path)\n",
    "else:\n",
    "    pitch_prior_assignments = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Coaching prior assignments \n",
    "\n",
    "matching_files = [\n",
    "    f for f in os.listdir(folder)\n",
    "    if f.startswith('Finalized coaching schedule') and f.endswith('.xlsx')\n",
    "]\n",
    "\n",
    "if matching_files:\n",
    "    most_recent_file = max(\n",
    "        matching_files,\n",
    "        key=lambda x: datetime.datetime.strptime(x.split('_')[1].split('.')[0], '%Y-%m-%d')\n",
    "    )\n",
    "    most_recent_path = os.path.join(folder, most_recent_file)\n",
    "    coach_prior_assignments = pd.read_excel(most_recent_path)\n",
    "else:\n",
    "    coach_prior_assignments = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge hardcodes and prior assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# QUERY LETTER CRITIQUES\n",
    "\n",
    "columns_to_keep = ['Email', 'pubname1', 'pubname2', 'timeslot_start']\n",
    "\n",
    "# Filter and rename for clarity\n",
    "hc = (\n",
    "    hardcoded_qlc[columns_to_keep].copy()\n",
    "    if 'hardcoded_qlc' in locals() and isinstance(hardcoded_qlc, pd.DataFrame) and not hardcoded_qlc.empty\n",
    "    else pd.DataFrame(columns=columns_to_keep)\n",
    ")\n",
    "\n",
    "prior = (\n",
    "    qlc_prior_assignments[columns_to_keep].copy()\n",
    "    if 'qlc_prior_assignments' in locals() and isinstance(qlc_prior_assignments, pd.DataFrame) and not qlc_prior_assignments.empty\n",
    "    else pd.DataFrame(columns=columns_to_keep)\n",
    ")\n",
    "\n",
    "if not hc.empty or not prior.empty:\n",
    "    # Merge on identifying columns\n",
    "    merged = pd.merge(\n",
    "        hc,\n",
    "        prior,\n",
    "        on=['Email', 'pubname1', 'pubname2'],\n",
    "        how='outer',\n",
    "        suffixes=('_hc', '_prior')\n",
    "    )\n",
    "\n",
    "    # Use hardcoded time if present, otherwise fallback to prior assignment\n",
    "    merged['timeslot_start'] = merged['timeslot_start_hc'].combine_first(merged['timeslot_start_prior'])\n",
    "\n",
    "    # Keep only the final form\n",
    "    qlc_allrequests = merged[['Email', 'pubname1', 'pubname2', 'timeslot_start']].drop_duplicates()\n",
    "    del(merged)\n",
    "\n",
    "else:\n",
    "    print(\"No QLC data available to combine.\")\n",
    "del(hc, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# MS CRITIQUES\n",
    "columns_to_keep = ['Email', 'publisher', 'timeslot_start']\n",
    "\n",
    "# Filter and rename for clarity\n",
    "hc = (\n",
    "    hardcoded_ms[columns_to_keep].copy()\n",
    "    if 'hardcoded_ms' in locals() and isinstance(hardcoded_ms, pd.DataFrame) and not hardcoded_ms.empty\n",
    "    else pd.DataFrame(columns=columns_to_keep)\n",
    ")\n",
    "\n",
    "prior = (\n",
    "    ms_prior_assignments[columns_to_keep].copy()\n",
    "    if 'ms_prior_assignments' in locals() and isinstance(ms_prior_assignments, pd.DataFrame) and not ms_prior_assignments.empty\n",
    "    else pd.DataFrame(columns=columns_to_keep)\n",
    ")\n",
    "\n",
    "if not hc.empty or not prior.empty:\n",
    "    # Merge on identifying columns\n",
    "    merged = pd.merge(\n",
    "        hc,\n",
    "        prior,\n",
    "        on=['Email', 'publisher'],\n",
    "        how='outer',\n",
    "        suffixes=('_hc', '_prior')\n",
    "    )\n",
    "\n",
    "    # Use hardcoded time if present, otherwise fallback to prior assignment\n",
    "    merged['timeslot_start'] = merged['timeslot_start_hc'].combine_first(merged['timeslot_start_prior'])\n",
    "\n",
    "    # Keep only the final form\n",
    "    ms_allrequests = merged[columns_to_keep].drop_duplicates()\n",
    "    del(merged)\n",
    "\n",
    "else:\n",
    "    print(\"No MS data available to combine.\")\n",
    "del(hc, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# PITCHES\n",
    "columns_to_keep = ['Email', 'publisher', 'timeslot_start']\n",
    "\n",
    "# Filter and rename for clarity\n",
    "hc = (\n",
    "    hardcoded_pitch[columns_to_keep].copy()\n",
    "    if 'hardcoded_pitch' in locals() and isinstance(hardcoded_pitch, pd.DataFrame) and not hardcoded_pitch.empty\n",
    "    else pd.DataFrame(columns=columns_to_keep)\n",
    ")\n",
    "\n",
    "prior = (\n",
    "    pitch_prior_assignments[columns_to_keep].copy()\n",
    "    if 'pitch_prior_assignments' in locals() and isinstance(pitch_prior_assignments, pd.DataFrame) and not pitch_prior_assignments.empty\n",
    "    else pd.DataFrame(columns=columns_to_keep)\n",
    ")\n",
    "\n",
    "if not hc.empty or not prior.empty:\n",
    "    # Merge on identifying columns\n",
    "    merged = pd.merge(\n",
    "        hc,\n",
    "        prior,\n",
    "        on=['Email', 'publisher'],\n",
    "        how='outer',\n",
    "        suffixes=('_hc', '_prior')\n",
    "    )\n",
    "\n",
    "    # Use hardcoded time if present, otherwise fallback to prior assignment\n",
    "    merged['timeslot_start'] = merged['timeslot_start_hc'].combine_first(merged['timeslot_start_prior'])\n",
    "\n",
    "    # Keep only the final form\n",
    "    pitch_allrequests = merged[columns_to_keep].drop_duplicates()\n",
    "    del(merged)\n",
    "\n",
    "else:\n",
    "    print(\"No pitch data available to combine.\")\n",
    "del(hc, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# COACHES\n",
    "columns_to_keep = ['Email', 'publisher', 'timeslot_start']\n",
    "\n",
    "# Filter and rename for clarity\n",
    "hc = (\n",
    "    hardcoded_coach[columns_to_keep].copy()\n",
    "    if 'hardcoded_coach' in locals() and isinstance(hardcoded_coach, pd.DataFrame) and not hardcoded_coach.empty\n",
    "    else pd.DataFrame(columns=columns_to_keep)\n",
    ")\n",
    "\n",
    "prior = (\n",
    "    coach_prior_assignments[columns_to_keep].copy()\n",
    "    if 'coach_prior_assignments' in locals() and isinstance(coach_prior_assignments, pd.DataFrame) and not coach_prior_assignments.empty\n",
    "    else pd.DataFrame(columns=columns_to_keep)\n",
    ")\n",
    "\n",
    "if not hc.empty or not prior.empty:\n",
    "    # Merge on identifying columns\n",
    "    merged = pd.merge(\n",
    "        hc,\n",
    "        prior,\n",
    "        on=['Email', 'publisher'],\n",
    "        how='outer',\n",
    "        suffixes=('_hc', '_prior')\n",
    "    )\n",
    "\n",
    "    # Use hardcoded time if present, otherwise fallback to prior assignment\n",
    "    merged['timeslot_start'] = merged['timeslot_start_hc'].combine_first(merged['timeslot_start_prior'])\n",
    "\n",
    "    # Keep only the final form\n",
    "    coach_allrequests = merged[columns_to_keep].drop_duplicates()\n",
    "    del(merged)\n",
    "else:\n",
    "    print(\"No coach data available to combine.\")\n",
    "del(hc, prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for changes in registration that affects prior scheduling/requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to double check that nobody's changed their registration - if so, we want to DROP them from the allrequests datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Check for MS critique changes\n",
    "\n",
    "if 'ms_allrequests' in locals() and not ms_allrequests.empty: # We only want to run this code IF the ms_allrequests dataset exists\n",
    "    # Step 1: Melt ms_critiques to long format\n",
    "    ms_long = ms_critiques.melt(id_vars='Email', value_vars=['msA', 'msB', 'msC'], \n",
    "                                var_name='ms_slot', value_name='publisher')\n",
    "\n",
    "    # Step 2: Drop rows with missing publishers (in case some msA/B/C are blank)\n",
    "    ms_long = ms_long.dropna(subset=['publisher'])\n",
    "\n",
    "    # Step 3: Merge to keep only rows from ms_allrequests that match email + publisher\n",
    "    filtered_ms_allrequests = pd.merge(ms_allrequests, ms_long[['Email', 'publisher']], \n",
    "                                       on=['Email', 'publisher'], how='inner')\n",
    "\n",
    "    del ms_long, ms_allrequests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Check for pitch changes\n",
    "\n",
    "if 'pitch_allrequests' in locals() and not pitch_allrequests.empty: # We only want to run this code IF the pitch_allrequests dataset exists\n",
    "    # Step 1: Melt ms_critiques to long format\n",
    "    pitch_long = pitches.melt(id_vars='Email', value_vars=['pitchA', 'pitchB', 'pitchC'], \n",
    "                                var_name='pitch_slot', value_name='publisher')\n",
    "\n",
    "    # Step 2: Drop rows with missing publishers (in case some msA/B/C are blank)\n",
    "    pitch_long = pitch_long.dropna(subset=['publisher'])\n",
    "\n",
    "    # Step 3: Merge to keep only rows from ms_allrequests that match email + publisher\n",
    "    filtered_pitch_allrequests = pd.merge(pitch_allrequests, pitch_long[['Email', 'publisher']], \n",
    "                                    on=['Email', 'publisher'], how='inner')\n",
    "\n",
    "    del(pitch_long, pitch_allrequests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Email, actual_requests, num_query_critiques]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check for QLC changes\n",
    "\n",
    "if not qlc_allrequests.empty: # We only want to run this code IF the qlc_allrequests dataset exists\n",
    "    # Step 1: Count how many requests each email made in qlc_allrequests\n",
    "    request_counts = qlc_allrequests['Email'].value_counts().reset_index()\n",
    "    request_counts.columns = ['Email', 'actual_requests']\n",
    "\n",
    "    # Step 2: Merge that with query_critiques to compare with allowed requests\n",
    "    comparison = pd.merge(request_counts, query_critiques[['Email', 'num_query_critiques']], on='Email', how='inner')\n",
    "\n",
    "    # Step 3: Filter for only those emails where actual ≤ allowed\n",
    "    valid_emails = comparison[comparison['actual_requests'] <= comparison['num_query_critiques']]['Email']\n",
    "\n",
    "    # Step 4: Filter qlc_allrequests to keep only valid emails\n",
    "    filtered_qlc_allrequests = qlc_allrequests[qlc_allrequests['Email'].isin(valid_emails)]\n",
    "\n",
    "    # Just to manually check, print any rows in the comparison dataset where actual_requests != num_query_critiques\n",
    "    print(comparison.loc[comparison['actual_requests'] != comparison['num_query_critiques']],)\n",
    "\n",
    "    del(request_counts, comparison, valid_emails, qlc_allrequests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Check for Coach changes\n",
    "\n",
    "if 'coach_allrequests' in locals() and not coach_allrequests.empty: # We only want to run this code IF the coach_allrequests dataset exists\n",
    "    # Step 1: Melt to long format\n",
    "    coach_long = reg_coaching.melt(id_vars='Email', value_vars=['Coach'], \n",
    "                                var_name='coach_slot', value_name='publisher')\n",
    "\n",
    "    # Step 2: Drop rows with missing publishers (in case some msA/B/C are blank)\n",
    "    coach_long = coach_long.dropna(subset=['publisher'])\n",
    "\n",
    "    # Step 3: Merge to keep only rows from ms_allrequests that match email + publisher\n",
    "    filtered_coach_allrequests = pd.merge(coach_allrequests, coach_long[['Email', 'publisher']], \n",
    "                                    on=['Email', 'publisher'], how='inner')\n",
    "\n",
    "    del(coach_long, coach_allrequests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! This should work now. Awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Schedule Friday QLCs\n",
    "\n",
    "We need to add in the requests and prior assignments information to the query_critiques dataset, so requests and prior assignments can be accounted for when scheduling. Please note that even if every single person has already been scheduled, the code should still run, and will just schedule every single person to exactly where they were previously assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Now let's merge it with the filtered_qlc_allrequests dataset (assuming it exists)\n",
    "if 'filtered_qlc_allrequests' in locals() and not filtered_qlc_allrequests.empty: # We only want to run this code IF the qlc_allrequests dataset exists\n",
    "    merged = pd.merge(query_critiques, filtered_qlc_allrequests, on='Email', how='left').drop_duplicates()\n",
    "else:\n",
    "    merged = query_critiques\n",
    "    merged['pubname1'] = np.nan\n",
    "    merged['pubname2'] = np.nan\n",
    "    merged['timeslot_start'] = pd.NaT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "expanded = []\n",
    "\n",
    "# First, get the number of rows per email\n",
    "email_counts = merged['Email'].value_counts()\n",
    "\n",
    "for email, count in email_counts.items():\n",
    "\n",
    "    # Iterate through each participant (e.g., by email)\n",
    "    email_rows = merged[merged['Email'] == email]\n",
    "\n",
    "    # If there's only one row, keep as is\n",
    "    if email_rows['num_query_critiques'].iloc[0] == 1:\n",
    "        expanded.append(email_rows.iloc[0])\n",
    "    # If the number of query critiques ==2\n",
    "    elif email_rows['num_query_critiques'].iloc[0] == 2:\n",
    "        if count == 2: # If already has two rows (because made two requests), keep the two rows as is\n",
    "            expanded.extend(email_rows.values)\n",
    "        elif count ==1: # If only have one request but paid for two meetings\n",
    "            expanded.append(email_rows.iloc[0])\n",
    "            # Add a 2nd row with blank values for pubname1, pubname2 and timeslot_start\n",
    "            row_copy = email_rows.iloc[0].copy()\n",
    "            row_copy['pubname1'] = np.nan\n",
    "            row_copy['pubname2'] = np.nan\n",
    "            row_copy['timeslot_start'] = pd.NaT\n",
    "            expanded.append(row_copy)\n",
    "\n",
    "expanded_query_critiques = pd.DataFrame(expanded)\n",
    "expanded_query_critiques.columns = merged.columns\n",
    "\n",
    "del(expanded, merged, query_critiques, filtered_qlc_allrequests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get frequencies of fiction and nonfiction genres among registrants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to scheduling, there's ONE final step: getting the different combinations of fiction and nonfiction genres among our registrants, to see which are most popular/least popular, so we can do our best to match agent-editor pairings that will meet everyone's needs.\n",
    "\n",
    "We likely won't do much with this information, but it's nice to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def count_genres(row):\n",
    "    genres = row['Fiction genre'].split(', ')\n",
    "    unique_genres = set(genres)\n",
    "    for genre in unique_genres:\n",
    "        genre_counts[genre] = genre_counts.get(genre, 0) + 1\n",
    "\n",
    "# Initialize genre_counts\n",
    "genre_counts = {}\n",
    "\n",
    "# Apply the function to each row\n",
    "per_registrant2.apply(count_genres, axis=1)\n",
    "\n",
    "# Convert genre_counts to DataFrame\n",
    "reg_fict_counts = pd.DataFrame(list(genre_counts.items()), columns=['fiction', 'registrant_fiction_counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(genre_counts, count_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def count_genres(row):\n",
    "    genres = row['Nonfiction genre'].split(', ')\n",
    "    unique_genres = set(genres)\n",
    "    for genre in unique_genres:\n",
    "        genre_counts[genre] = genre_counts.get(genre, 0) + 1\n",
    "\n",
    "# Initialize genre_counts\n",
    "genre_counts = {}\n",
    "\n",
    "# Apply the function to each row\n",
    "per_registrant2.apply(count_genres, axis=1)\n",
    "\n",
    "# Convert genre_counts to DataFrame\n",
    "reg_nonfict_counts = pd.DataFrame(list(genre_counts.items()), columns=['nonfiction', 'registrant_nonfiction_counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of some weirdness with the other writeins (for instance \"Other (please specify): songs, movie and tv scripts\"), there are some nonfiction genres popping up that shouldn't be. We'll filter out anything that isn't our true genre lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "reg_nonfict_counts = reg_nonfict_counts.loc[reg_nonfict_counts['nonfiction'].isin(nonfict_gen['list_nonfiction']), :]\n",
    "reg_fict_counts = reg_fict_counts.loc[reg_fict_counts['fiction'].isin(fict_gen['fiction_genres']), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, let's now get expanded counts for each of the types in the cross_pubs listing too, so we can cross-tabulate that with the datasets above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def count_genres(row):\n",
    "    genres = row['combined_fiction'].split(', ')\n",
    "    unique_genres = set(genres)\n",
    "    for genre in unique_genres:\n",
    "        genre_counts[genre] = genre_counts.get(genre, 0) + 1\n",
    "\n",
    "# Initialize genre_counts\n",
    "genre_counts = {}\n",
    "\n",
    "# Apply the function to each row\n",
    "final_cross_pubs.apply(count_genres, axis=1)\n",
    "\n",
    "# Convert genre_counts to DataFrame\n",
    "fiction_counts = pd.DataFrame(list(genre_counts.items()), columns=['fiction', 'fiction_count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(count_genres, genre_counts, nonfict_gen, fict_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def count_genres(row):\n",
    "    genres = row['combined_nonfiction'].split(', ')\n",
    "    unique_genres = set(genres)\n",
    "    for genre in unique_genres:\n",
    "        genre_counts[genre] = genre_counts.get(genre, 0) + 1\n",
    "\n",
    "# Initialize genre_counts\n",
    "genre_counts = {}\n",
    "\n",
    "# Apply the function to each row\n",
    "cross_pubs_filtered = final_cross_pubs[final_cross_pubs['combined_nonfiction'].notnull()] \n",
    "cross_pubs_filtered.apply(count_genres, axis=1)\n",
    "\n",
    "# Convert genre_counts to DataFrame\n",
    "nonfiction_counts = pd.DataFrame(list(genre_counts.items()), columns=['nonfiction', 'nonfiction_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a dataset with the participant, as well as publisher-pairing, genre count info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "all_fiction_genre_info = pd.merge(fiction_counts, reg_fict_counts, how='outer', on='fiction')\n",
    "all_nonfiction_genre_info = pd.merge(nonfiction_counts, reg_nonfict_counts, how='outer', on='nonfiction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "all_fiction_genre_info.to_excel(f\"{current_conference_folder}/Outputs/Frequencies_fiction.xlsx\", index=False)\n",
    "all_nonfiction_genre_info.to_excel(f\"{current_conference_folder}/Outputs/Frequencies_nonfiction.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew! We're finally done with that. You can manually review the files output above to help figure out the agent-editor pairings, or double check them, or just to see which genres in fiction and nonfiction are really popular. But otherwise, we're good to move on to actually assigning participants to their Friday timeslots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign participants to their Friday timeslots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the prior sections of code, we created:\n",
    "\n",
    "1) The rooms for Friday with the agents/editors assigned to them (final_room_pairings_Friday)\n",
    "2) The list of all time slots and rooms for Friday (tslist_fri)\n",
    "3) The list of all participants who signed up for a query letter critique (expanded_query_critiques), which has multiple rows per person - one for the number of queries they signed up for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on though, let's join #1 and #2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "times_friday = tslist_fri.merge(final_room_pairings_Friday, on=['day', 'room_name'], how='outer').sort_values(['timeslot_start', 'room_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also bring in the information on what combined genres those publisher pairings represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "times_friday2 = times_friday.merge(final_cross_pubs[['pubname1', 'pubname2', 'combined_fiction', 'combined_nonfiction']], on=['pubname1', 'pubname2'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(tslist_fri, times_friday, reg_nonfict_counts, reg_fict_counts, rooms_friday, nonfiction_counts, fiction_counts, all_fiction_genre_info, all_nonfiction_genre_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! Now let's begin the assignments.\n",
    "\n",
    "In this section of code, we will assign participants to an agent-editor pairing for whom they're not pitching or doing a manuscript critique for (if any), and that represents at least one of the genres (fiction and/or nonfiction) that the registrant writes in.\n",
    "\n",
    "Note that we will not schedule anyone back-to-back, that we will prioritize virtual people for the first sessions (and prioritize any virtual people to be followed by virtual people), and that we will also prioritize anyone who signed up for the Friday workshop for the earlier sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# First, we need to create lists for anything separated by a comma\n",
    "times_friday2['combined_nonfiction'] = times_friday2['combined_nonfiction'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])\n",
    "times_friday2['combined_fiction'] = times_friday2['combined_fiction'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])\n",
    "\n",
    "expanded_query_critiques['nonfiction_genre'] = expanded_query_critiques['Nonfiction genre'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])\n",
    "expanded_query_critiques['fiction_genre'] = expanded_query_critiques['Fiction genre'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])\n",
    "expanded_query_critiques['pitches_chosen_pubs'] = expanded_query_critiques['pitches_chosen_pubs'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])\n",
    "expanded_query_critiques['ms_chosen_pubs'] = expanded_query_critiques['ms_chosen_pubs'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Combine the publishers into a single list\n",
    "expanded_query_critiques['chosen_pubs'] = expanded_query_critiques['pitches_chosen_pubs'] + expanded_query_critiques['ms_chosen_pubs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Email, pubname1, counts]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Quick check  before we proceed - does anyone have duplicate assignments? As in, does the same email and room combo appear twice?\n",
    "check = expanded_query_critiques[['Email', 'pubname1']].value_counts().reset_index(name='counts')\n",
    "check = check[check['counts'] > 1]\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For assignment purposes, we're going to prioritize people according to the following:\n",
    "1) Virtual\n",
    "2) How many publishers they signed up with for manuscript critiques and/or pitches\n",
    "3) Friday workshop attendees\n",
    "4) Registration date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Sort participants by prioritization criteria\n",
    "expanded_query_critiques['chosen_pubs_count'] = expanded_query_critiques['chosen_pubs'].apply(len)\n",
    "expanded_query_critiques.sort_values(\n",
    "    by=['timeslot_start', 'pubname1', 'Virtual', 'chosen_pubs_count', 'Friday_workshop', 'datetime'],\n",
    "    ascending=[True, True, False, False, True, True],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the 'timeslot_start' to datetime variable\n",
    "times_friday2['timeslot_start'] = pd.to_datetime(date_str_fri + ' ' + times_friday2['timeslot_start'].astype(str))\n",
    "\n",
    "# Adjust the times to represent the afternoon (add 12 hours if in AM range)\n",
    "times_friday2['timeslot_start'] = times_friday2['timeslot_start'].apply(\n",
    "    lambda x: x + pd.Timedelta(hours=12) if x.hour < 12 else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if everyone has already been completely assigned, it should fill every slot with the very first seed (seed=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All participants successfully assigned with seed: 0\n"
     ]
    }
   ],
   "source": [
    "assignments = []\n",
    "\n",
    "# Create a new dataset with all the people:\n",
    "participants_df = expanded_query_critiques.copy()\n",
    "slots_df = times_friday2\n",
    "\n",
    "# Helper function to check slot compatibility\n",
    "def is_slot_compatible(participant, slot):\n",
    "\n",
    "    # Check if at least one genre matches\n",
    "    participant_fiction_genres = set(participant['fiction_genre'])\n",
    "    slot_fiction_genres = set(slot['combined_fiction'])\n",
    "\n",
    "    participant_nonfiction_genres = set(participant['nonfiction_genre'])\n",
    "    slot_nonfiction_genres = set(slot['combined_nonfiction'])\n",
    "\n",
    "    if not (participant_fiction_genres & slot_fiction_genres or participant_nonfiction_genres & slot_nonfiction_genres):\n",
    "        #print(f\"Incompatible due to genres.\") \n",
    "        return False\n",
    "\n",
    "    # Check publisher overlap\n",
    "    participant_pubs = set(participant['chosen_pubs'])\n",
    "    slot_pubs = {slot['pubname1'], slot['pubname2']}\n",
    "    if participant_pubs.intersection(slot_pubs):\n",
    "        #print(f\"Incompatible due to publisher overlap. Participant publishers: {participant_pubs}, Slot publishers: {slot_pubs}\")\n",
    "        return False\n",
    "\n",
    "    # Check if they're a workshop person and don't assign for 4pm or later\n",
    "    if participant['Friday_workshop'] and slot['timeslot_start'].hour >= 16:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# We need to create another helper function specifically for hardcoded request people. In that case, it doesn't matter if they have any genre overlap - we just need to not schedule them after 4pm if they're a workshop attendee\n",
    "def is_slot_compatible_requests(participant, slot):\n",
    "\n",
    "        # Check if they're a workshop person and don't assign for 4pm or later\n",
    "    if participant['Friday_workshop'] and slot['timeslot_start'].hour >= 16:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Define a function to perform the assignment process\n",
    "def assign_participants(participants_df, slots_df, seed):\n",
    "\n",
    "    # Shuffle slots_df with the given seed\n",
    "    randomized_slots = slots_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    assignments_local = []\n",
    "\n",
    "    # Create a copy of participants_df to modify\n",
    "    remaining_participants = participants_df.copy()\n",
    "\n",
    "    for _, slot in randomized_slots.iterrows():\n",
    "        if remaining_participants.empty:\n",
    "            break  # Exit if all participants are assigned\n",
    "\n",
    "        for index, participant in remaining_participants.iterrows():\n",
    "            # Skip participants already assigned to conflicting slots\n",
    "            assigned_slots = [a['timeslot_start'] for a in assignments_local if a['Email'] == participant['Email']]\n",
    "            if any(abs(slot['timeslot_start'] - assigned) <= timedelta(minutes=15) for assigned in assigned_slots):\n",
    "                continue\n",
    "\n",
    "            # Skip if participant has already been assigned to either of these publishers\n",
    "            participant_assignments = [a for a in assignments_local if a['Email'] == participant['Email']]\n",
    "            assigned_pubs = {a['pubname1'] for a in participant_assignments} | {a['pubname2'] for a in participant_assignments}\n",
    "\n",
    "            if slot['pubname1'] in assigned_pubs or slot['pubname2'] in assigned_pubs:\n",
    "                continue\n",
    "\n",
    "            requested_timeslots = participant.get('timeslot_start')\n",
    "\n",
    "            participant_p1 = participant.get('pubname1')\n",
    "            participant_p2 = participant.get('pubname2')\n",
    "            slot_p1 = slot['pubname1']\n",
    "            slot_p2 = slot['pubname2']\n",
    "\n",
    "            requested_publishers = {\n",
    "                pub for pub in [participant_p1, participant_p2] if pd.notna(pub)\n",
    "            }\n",
    "\n",
    "            slot_publishers = {pub for pub in [slot_p1, slot_p2] if pd.notna(pub)}\n",
    "\n",
    "            has_requested_publisher = bool(requested_publishers)  # True only if they actually requested publishers\n",
    "\n",
    "            if has_requested_publisher:  # If they requested ANY publisher\n",
    "                #print(f\"🔎 {participant['Email']} has requested publishers: {participant_p1}, {participant_p2}\")\n",
    "\n",
    "                # Case 1: If they requested a specific timeslot, enforce that too\n",
    "                if requested_publishers == slot_publishers and pd.notna(requested_timeslots) and requested_timeslots == slot['timeslot_start']:\n",
    "                    #print(f\"✅ Assigning {participant['Email']} to requested slot with {slot_p1}, {slot_p2} and start time\")\n",
    "\n",
    "                    assignments_local.append({\n",
    "                        'Email': participant['Email'],\n",
    "                        'timeslot_start': slot['timeslot_start'],\n",
    "                        'room_name': slot['room_name'],\n",
    "                        'pubname1': slot['pubname1'],\n",
    "                        'pubname2': slot['pubname2'],\n",
    "                        'participant_fiction_genre': ', '.join(participant['fiction_genre']),\n",
    "                        'participant_nonfiction_genre': ', '.join(participant['nonfiction_genre']),\n",
    "                        'publisher_fiction_genre': slot['combined_fiction'],\n",
    "                        'publisher_nonfiction_genre': slot['combined_nonfiction'],\n",
    "                        'workshop': participant['Friday_workshop'],\n",
    "                        'virtual': participant['Virtual']\n",
    "                    })\n",
    "                    remaining_participants.drop(index, inplace=True)\n",
    "                    break\n",
    "\n",
    "                # Case 2: If they didn't specify a time, assign them as long as publisher matches\n",
    "                elif requested_publishers == slot_publishers and pd.isna(requested_timeslots):  \n",
    "                    #print(\"Requested publishers but not a specific time\")\n",
    "                    if is_slot_compatible_requests(participant, slot):\n",
    "                        \n",
    "                        assignments_local.append({\n",
    "                            'Email': participant['Email'],\n",
    "                            'timeslot_start': slot['timeslot_start'],\n",
    "                            'room_name': slot['room_name'],\n",
    "                            'pubname1': slot['pubname1'],\n",
    "                            'pubname2': slot['pubname2'],\n",
    "                            'participant_fiction_genre': ', '.join(participant['fiction_genre']),\n",
    "                            'participant_nonfiction_genre': ', '.join(participant['nonfiction_genre']),\n",
    "                            'publisher_fiction_genre': slot['combined_fiction'],\n",
    "                            'publisher_nonfiction_genre': slot['combined_nonfiction'],\n",
    "                            'workshop': participant['Friday_workshop'],\n",
    "                            'virtual': participant['Virtual']\n",
    "                        })\n",
    "                        remaining_participants.drop(index, inplace=True)\n",
    "                        break\n",
    "                    else:\n",
    "                        #print(f\"Anybody showing up here?\")\n",
    "                        # IMPORTANT: Prevent assigning them if NO requested publishers match\n",
    "                        continue\n",
    "                else:\n",
    "                    #print(f\"What about here?\")\n",
    "                    continue  \n",
    "            else:\n",
    "                #print('Participant had no requests')\n",
    "                if is_slot_compatible(participant, slot):\n",
    "                    assignments_local.append({\n",
    "                        'Email': participant['Email'],\n",
    "                        'timeslot_start': slot['timeslot_start'],\n",
    "                        'room_name': slot['room_name'],\n",
    "                        'pubname1': slot['pubname1'],\n",
    "                        'pubname2': slot['pubname2'],\n",
    "                        'participant_fiction_genre': ', '.join(participant['fiction_genre']),\n",
    "                        'participant_nonfiction_genre': ', '.join(participant['nonfiction_genre']),\n",
    "                        'publisher_fiction_genre': slot['combined_fiction'],\n",
    "                        'publisher_nonfiction_genre': slot['combined_nonfiction'],\n",
    "                        'workshop': participant['Friday_workshop'],\n",
    "                        'virtual': participant['Virtual']\n",
    "                    })\n",
    "                    # Remove the assigned participant row\n",
    "                    remaining_participants.drop(index, inplace=True)\n",
    "                    break\n",
    "\n",
    "    return assignments_local, remaining_participants\n",
    "\n",
    "# Attempt to assign participants with different seeds until successful\n",
    "seed = 0\n",
    "\n",
    "while seed < 1000:\n",
    "    #print(f\"Trying seed: {seed}\")\n",
    "    assignments, remaining = assign_participants(participants_df, slots_df, seed)\n",
    "    \n",
    "    if remaining.empty:  # All participants assigned\n",
    "        print(f\"All participants successfully assigned with seed: {seed}\")\n",
    "        break\n",
    "\n",
    "    print(f\"{len(remaining)} Unassigned participants remain with seed: {seed}.\")\n",
    "    seed += 1  # Increment the seed for the next iteration\n",
    "\n",
    "# Convert assignments to a DataFrame\n",
    "assignments_df = pd.DataFrame(assignments)\n",
    "remaining = pd.DataFrame(remaining)\n",
    "del(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(participants_df))\n",
    "print(len(expanded_query_critiques)== len(assignments_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Email, Virtual, Fiction genre, Nonfiction genre, datetime, pitchA, pitchB, pitchC, msA, msB, msC, pitches_chosen_pubs, ms_chosen_pubs, num_query_critiques, query_critique, Friday_workshop, pubname1, pubname2, timeslot_start, nonfiction_genre, fiction_genre, chosen_pubs, chosen_pubs_count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check which people didn't get assigned to figure out why\n",
    "print(remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [pubname1, pubname2, counts]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check: which pairings are left?\n",
    "check = assignments_df[['pubname1', 'pubname2']].value_counts().reset_index(name='counts')\n",
    "check = check[check['counts'] <12]\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [pubname1, pubname2, counts]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check the originals before this further assignment\n",
    "check = expanded_query_critiques[['pubname1', 'pubname2']].value_counts().reset_index(name='counts')\n",
    "check = check[check['counts'] <12]\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! That looks amazing, and all participants got slotted! Let's just add in some checks to flag participants who have bad criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Flag participants with a Friday workshop and slots from 4-5pm\n",
    "assignments_df['Flag'] = assignments_df.apply(\n",
    "    lambda x: (x['workshop']) and (x['timeslot_start'].hour >= 16),  # After 4pm check\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flag\n",
      "False    108\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(assignments_df['Flag'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Email, timeslot_start, room_name, pubname1, pubname2, participant_fiction_genre, participant_nonfiction_genre, publisher_fiction_genre, publisher_nonfiction_genre, workshop, virtual, Flag]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Who are the people being assigned spots after 4pm that are also in the workshop?\n",
    "print(assignments_df.loc[assignments_df['Flag']==True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, awesome! We're good to go, and now let's just print out the csv file with all the assignments, and also save the dataset as a final (better named) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_friday_assignments = assignments_df\n",
    "\n",
    "del(assignments_df, participants_df, slots_df, is_slot_compatible_requests, clean_genres, combine_variables, count_genres,\n",
    "    is_slot_compatible, genre_counts, assignments, hardcoded_qlc, hardcoded, expanded_query_critiques, email_counts, email_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, create a column called 'publisher' that is a merging of the two publishers names, and also add a variable 'Session' that says 'Query Letter Critiques'. Oh, and add in a variable for 'Timekeeper', which is True/False depending on if the participant is a timekeeper that day or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_friday_assignments['publisher'] = final_friday_assignments['pubname1'] + \" and \" + final_friday_assignments['pubname2']\n",
    "final_friday_assignments['Session'] = \"Query Letter Critiques\"\n",
    "final_friday_assignments['Timekeeper'] = final_friday_assignments['Email'].isin(timekeepers['Email'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's link in the first and last names,as well as phone numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_friday_assignments2 = pd.merge(final_friday_assignments, registered[['Email', 'First Name', 'Last Name', 'phone']].drop_duplicates(), on=\"Email\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Email, pubname1, counts]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Quick final check before we save the dataset - does anyone have duplicate assignments? As in, does the same email and room combo appear twice?\n",
    "check = final_friday_assignments2[['Email', 'pubname1']].value_counts().reset_index(name='counts')\n",
    "check = check[check['counts'] > 1]\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_friday_assignments2.to_excel(f\"{current_conference_folder}/Outputs/Finalized datasets/Final Friday query letter critique assignments_{today}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet. Now we're good to go on Friday, so let's move on to Saturday assignments, which are easier. For these, the publishers have already been assigned to their own rooms, and our participants signed up to meet with specific agents and editors, so we just need to assign them to specific times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Friday Author Coaching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we need to update the timeslots thing so that they are datetimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the 'timeslot_start' to datetime variable\n",
    "tslist_coach['timeslot_start'] = pd.to_datetime(date_str_fri + ' ' + tslist_coach['timeslot_start'].astype(str))\n",
    "\n",
    "# Adjust the times to represent the afternoon (add 12 hours if in AM range)\n",
    "tslist_coach['timeslot_start'] = tslist_coach['timeslot_start'].apply(\n",
    "    lambda x: x + pd.Timedelta(hours=12) if x.hour < 12 else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# We need to create an alternate version of the friday assignments dataset so that we can add buffer times for our purposes here:\n",
    "publisher_meetings = final_friday_assignments2.copy()\n",
    "publisher_meetings['timeslot_end'] = publisher_meetings['timeslot_start'] + timedelta(minutes=15)\n",
    "publisher_meetings['buffer_start'] = publisher_meetings['timeslot_start'] - timedelta(minutes=15)\n",
    "publisher_meetings['buffer_end'] = publisher_meetings['timeslot_end'] + timedelta(minutes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# We also need to add end times for the coaching meetings. Note that they're technically 15 minutes, but we'll ignore that and pretend they're 17 since there's a two minute break between them\n",
    "tslist_coach['timeslot_end'] = tslist_coach['timeslot_start'] + timedelta(minutes=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We to transform the publisher_meetings dataset so that it has one row per participant. This way we can merge it with the coaching list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Add a count per Email to identify qlc1 and qlc2\n",
    "publisher_meetings['qlc_number'] = publisher_meetings.groupby('Email').cumcount() + 1\n",
    "\n",
    "# Pivot the table to wide format\n",
    "qlc_times_to_exclude = publisher_meetings.pivot(index='Email', columns='qlc_number', values=['buffer_start', 'buffer_end'])\n",
    "\n",
    "# Flatten the MultiIndex columns\n",
    "qlc_times_to_exclude.columns = [f\"qlc{col[1]}_{col[0]}\" for col in qlc_times_to_exclude.columns]\n",
    "\n",
    "# Reset index to make 'Email' a column again\n",
    "qlc_times_to_exclude = qlc_times_to_exclude.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Merge this data with the coaching dataset\n",
    "reg_coaching2 = pd.merge(reg_coaching, qlc_times_to_exclude[['Email', 'qlc1_buffer_start', 'qlc1_buffer_end', 'qlc2_buffer_start', 'qlc2_buffer_end']], how=\"left\", on=\"Email\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any hardcoded or prior requests, merge in this info with the reg_coaching2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Now let's merge it with the filtered_qlc_allrequests dataset (assuming it exists)\n",
    "if 'filtered_coach_allrequests' in locals() and not filtered_coach_allrequests.empty: # We only want to run this code IF the qlc_allrequests dataset exists\n",
    "    coach_toassign = pd.merge(reg_coaching2, filtered_coach_allrequests, on='Email', how='left').drop_duplicates()\n",
    "else:\n",
    "    coach_toassign = reg_coaching2\n",
    "    coach_toassign['publisher'] = np.nan\n",
    "    coach_toassign['timeslot_start'] = pd.NaT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(reg_coaching, qlc_times_to_exclude, publisher_meetings, reg_coaching2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Lastly, we also need to add a flag for whether or not the person is attending the Q&A\n",
    "fri_qa_panel = registered.loc[registered['Agenda Item Name'].str.contains(\"Q&A\")]\n",
    "coach_toassign['QA_panel'] = coach_toassign['Email'].isin(fri_qa_panel['Email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Sort participants by prioritization criteria\n",
    "coach_toassign.sort_values(\n",
    "    by=['timeslot_start', 'publisher', 'QLC', 'Virtual', 'QA_panel', 'Friday_workshop'],\n",
    "    ascending=[True, True, False, False, False, False],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All participants successfully assigned with seed: 0\n"
     ]
    }
   ],
   "source": [
    "coaching_timeslots = tslist_coach.copy()\n",
    "participants_df = coach_toassign.copy()\n",
    "from datetime import time\n",
    "\n",
    "# ---- Helper function to check slot compatibility\n",
    "def compatible_with_slot(participant, slot):\n",
    "\n",
    "    # 1a. Check if they're a workshop person and don't assign to any of the timeslots that'd run over into the workshop time\n",
    "    if participant['Friday_workshop'] and slot['timeslot_start'].time() >= time(15, 45):\n",
    "        return False\n",
    "\n",
    "    # 1b. Check if they're part of the Q&A panel (doing 1:40 so they have time to get there 15 mins in advance)\n",
    "    if participant['QA_panel'] and time(12,20) <= slot['timeslot_start'].time() <= time(13, 40):\n",
    "        return False\n",
    "\n",
    "    # 2. Check whether the slot's timeslot_start conflict with Query Letter Critique meetings\n",
    "    slot_end = slot['timeslot_start'] + timedelta(minutes=17)    \n",
    "    \n",
    "    # Check QLC1 conflict\n",
    "    qlc1_conflict = (\n",
    "        pd.notna(participant['qlc1_buffer_start']) and\n",
    "        pd.notna(participant['qlc1_buffer_end']) and\n",
    "        slot['timeslot_start'] < participant['qlc1_buffer_end'] and\n",
    "        slot_end > participant['qlc1_buffer_start']\n",
    "    )\n",
    "\n",
    "    # Check QLC2 conflict\n",
    "    qlc2_conflict = (\n",
    "        pd.notna(participant['qlc2_buffer_start']) and\n",
    "        pd.notna(participant['qlc2_buffer_end']) and\n",
    "        slot['timeslot_start'] < participant['qlc2_buffer_end'] and\n",
    "        slot_end > participant['qlc2_buffer_start']\n",
    "    )\n",
    "\n",
    "    if qlc1_conflict or qlc2_conflict:\n",
    "        return False\n",
    "\n",
    "    # 3. Lastly, and most obviously, make sure that the selected publisher is the same as the slot being evaluated\n",
    "    if participant['Coach'] != slot['coach']:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# ----- Now run the actual code to assign people\n",
    "def assign_coaching_meetings(participants_df, slots_df, seed):\n",
    "\n",
    "    # Shuffle timeslots with the given seed\n",
    "    randomized_timeslots = slots_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    coaching_schedule = []\n",
    "    remaining_participants = participants_df.copy()\n",
    "\n",
    "    for _, slot in randomized_timeslots.iterrows():\n",
    "        if remaining_participants.empty:\n",
    "            break  # Exit if all participants are assigned\n",
    "\n",
    "        for index, participant in remaining_participants.iterrows():\n",
    "\n",
    "            # Skip participants already assigned to conflicting slots and move to checking the next participant for compatibility\n",
    "            assigned_slots = [a['timeslot_start'] for a in coaching_schedule if a['Email'] == participant['Email']]\n",
    "            if any(abs(slot['timeslot_start'] - assigned) <= timedelta(minutes=17) for assigned in assigned_slots):\n",
    "                continue\n",
    "\n",
    "            # Check for any requests or prior assignments\n",
    "            requested_timeslots = participant.get('timeslot_start')\n",
    "            requested_coach = participant.get('publisher')\n",
    "\n",
    "            # If they have requests or prior assignments...\n",
    "            if pd.notna(requested_coach):\n",
    "                \n",
    "                # Scenario 1: They requested or were assigned to a specific timeslot. Check that the time matches, then assign them if so\n",
    "                if (\n",
    "                    requested_coach == slot['coach'] and\n",
    "                    pd.notna(requested_timeslots) and \n",
    "                    requested_timeslots == slot['timeslot_start']\n",
    "                ):\n",
    "                    coaching_schedule.append({\n",
    "                        'Email': participant['Email'],\n",
    "                        'timeslot_start': slot['timeslot_start'],\n",
    "                        'publisher': slot['coach'],\n",
    "                        'room_name': slot['room_name']\n",
    "                    })\n",
    "                    # Remove the assigned participant row\n",
    "                    remaining_participants.drop(index, inplace=True)\n",
    "                    break\n",
    "\n",
    "                # Scenario 2: They requested a specific coach but didn't care when. Check that they don't have problems with the workshop, then assign\n",
    "                elif (\n",
    "                    requested_coach == slot['coach'] and \n",
    "                    pd.isna(requested_timeslots) and\n",
    "                    compatible_with_slot(participant, slot)\n",
    "                ):\n",
    "                    coaching_schedule.append({\n",
    "                        'Email': participant['Email'],\n",
    "                        'timeslot_start': slot['timeslot_start'],\n",
    "                        'publisher': slot['coach'],\n",
    "                        'room_name': slot['room_name']\n",
    "                    })\n",
    "                    # Remove the assigned participant row\n",
    "                    remaining_participants.drop(index, inplace=True)\n",
    "                    break\n",
    "\n",
    "                # Scenario 3: They requested a specific coach, but the slot's coach is the wrong one, OR they didn't meet slot compatibility - move to next participant\n",
    "                #else:\n",
    "                #    continue\n",
    "\n",
    "            # If they don't have any requests or prior assignments:\n",
    "            elif pd.isna(requested_coach):\n",
    "                if compatible_with_slot(participant, slot):\n",
    "                    coaching_schedule.append({\n",
    "                        'Email': participant['Email'],\n",
    "                        'timeslot_start': slot['timeslot_start'],\n",
    "                        'publisher': slot['coach'],\n",
    "                        'room_name': slot['room_name']\n",
    "                    })\n",
    "                    # Remove the assigned participant row\n",
    "                    remaining_participants.drop(index, inplace=True)\n",
    "                    break\n",
    "\n",
    "                # If the timeslot and coach doesn't meet their requirements, then move to next participant\n",
    "                #else:\n",
    "                #    continue\n",
    "\n",
    "            #else: # Probably don't need this one, but just in case\n",
    "            #    continue     \n",
    "\n",
    "    return coaching_schedule, remaining_participants\n",
    "\n",
    "\n",
    "# Attempt to assign participants with different seeds until successful\n",
    "seed = 0\n",
    "\n",
    "while seed < 1000:\n",
    "    #print(f\"Trying seed: {seed}\")\n",
    "    assignments, remaining = assign_coaching_meetings(participants_df, coaching_timeslots, seed)\n",
    "    \n",
    "    if remaining.empty :  # All participants assigned\n",
    "        print(f\"All participants successfully assigned with seed: {seed}\")\n",
    "        break\n",
    "\n",
    "    print(f\"{len(remaining)} Unassigned participants remain with seed: {seed}, retrying...\")\n",
    "    seed += 1  # Increment the seed for the next iteration\n",
    "\n",
    "# Convert assignments to a DataFrame\n",
    "assignments_df = pd.DataFrame(assignments)\n",
    "remaining = pd.DataFrame(remaining)\n",
    "del(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the numbers match\n",
    "len(assignments_df) == len(coach_toassign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(coaching_timeslots, assign_coaching_meetings, assign_participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Add in the first and last names\n",
    "final_coaching = pd.merge(assignments_df, registered[['Email', 'First Name', 'Last Name', 'phone']].drop_duplicates(), on=\"Email\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Save this scheduling\n",
    "final_coaching.to_excel(f\"{current_conference_folder}/Outputs/Finalized datasets/Finalized coaching schedule_{today}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Manuscript critiques scheduling\n",
    "This is the easiest scheduling assignment. Everyone has already signed up for their critiques, so we just need to make sure:\n",
    "\n",
    "1) Nobody is scheduled back-to-back (for anyone with multiple)\n",
    "2) Timekeepers aren't first or last\n",
    "3) Virtual participants are grouped back-to-back (we will prioritize them for the first time slots per room).\n",
    "4) Any manual requests or prior scheduling assignments are applied\n",
    "\n",
    "Ideally, we also try to ensure that there's only one substitute for each time slot, though we have plenty of substitute timekeepers. This can just be a manual check and fix later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Assign the individual publishers to their respective rooms for Saturday morning (MS) and Saturday afternoon (Pitches)\n",
    "times_sat = pd.concat([rooms_saturday.reset_index(drop=True), pubs.reset_index(drop=True)], axis=1)\n",
    "\n",
    "tslist_satmorn= pd.merge(timeslots.loc[(timeslots['day']=='Saturday') & (timeslots['day_session']=='Morning'), :], times_sat, how='outer', on='day')\n",
    "tslist_sataft= pd.merge(timeslots.loc[(timeslots['day']=='Saturday') & (timeslots['day_session']=='Afternoon'), :], times_sat, how='outer', on='day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_saturday_rooms = pd.concat([rooms_saturday.reset_index(drop=True), pubs.reset_index(drop=True)], axis=1)\n",
    "final_saturday_rooms.to_excel(f\"{current_conference_folder}/Outputs/Finalized datasets/Final saturday rooms.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a single dataset for the manuscript critiques where every person has a row for their critique (as in, a person can have up to three rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(msA, msB, msC, pitchA, pitchB, pitchC) # delete these - had originally kept for this but need virtual info\n",
    "msA = ms_critiques[['Email', 'Virtual', 'msA']]\n",
    "msB = ms_critiques[['Email', 'Virtual', 'msB']]\n",
    "msC = ms_critiques[['Email', 'Virtual', 'msC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "msA = msA.rename(columns={'msA': 'publisher'})\n",
    "msB = msB.rename(columns={'msB': 'publisher'})\n",
    "msC = msC.rename(columns={'msC': 'publisher'})\n",
    "\n",
    "ms_all = pd.merge(pd.merge(msA, msB, on=['Email', 'Virtual', 'publisher'], how=\"outer\"), msC, on=['Email', 'Virtual', 'publisher'], how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop any rows with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ms_all = ms_all.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(msA, msB, msC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we just need to convert the timeslot_start to a timestamp variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the 'timeslot_start' to datetime variable\n",
    "tslist_satmorn['timeslot_start'] = pd.to_datetime(date_str_sat + ' ' + tslist_satmorn['timeslot_start'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the 'timeslot_start' to datetime variable\n",
    "tslist_sataft['timeslot_start'] = pd.to_datetime(date_str_sat + ' ' + tslist_sataft['timeslot_start'].astype(str))\n",
    "\n",
    "# Adjust the times to represent the afternoon (add 12 hours if in AM range)\n",
    "tslist_sataft['timeslot_start'] = tslist_sataft['timeslot_start'].apply(\n",
    "    lambda x: x + pd.Timedelta(hours=12) if x.hour < 12 else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also identify the timekeepers' emails. We'll make sure not to give them the first or last time slot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "timekeeps = timekeepers[['Email']].drop_duplicates()\n",
    "timekeeps = timekeeps['Email'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to add in any hardcoded or prior requests information to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Now let's merge it with the filtered_qlc_allrequests dataset (assuming it exists)\n",
    "if 'filtered_ms_allrequests' in locals() and not filtered_ms_allrequests.empty: # We only want to run this code IF the qlc_allrequests dataset exists\n",
    "    ms_all_withrequests = pd.merge(ms_all, filtered_ms_allrequests, on=['Email', 'publisher'], how='left').drop_duplicates()\n",
    "else:\n",
    "    ms_all_withrequests = ms_all\n",
    "    ms_all_withrequests['timeslot_start'] = pd.NaT # Only thing you can request is the timeslot, since you paid for an MS with a specific publisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew! Okay, now it's time to assign the participants for the manuscript critiques. The code below works by:\n",
    "\n",
    "1) It fills alphabetically by the publisher name, so that Alexandria Brown gets all her timeslots filled first, before moving on to the next publisher in the alphabet. **NOTE**: I have it randomly filling time slots. It's not running by earliest time to latest time.\n",
    "\n",
    "2) It prioritizes assignment of participants according to how many manuscript critique slots they still need to be assigned. This means that for the first time slot it tries filling, it'll prioritize people with 3 critiques, then 2, then 1. As it continues to iterate and participants get assigned slots, a participant who initially had 3 meetings but who was already scheduled for 2 (meaning n_remaining=1) will get less priority over participants still with 2 or three meetings needing assignment.\n",
    "\n",
    "3) I randomly shuffled the participants within their priority groups. This means that participant emails are randomly ordered in the A) three remaining group, B) two remaining and C) one remaining group. This way we don't prioritize people according to the alphabetical ordering of their emails but just do random assignments. (I had implemented this because I had noticed initially that a lot of the T-Z emails weren't being assigned as readily).\n",
    "\n",
    "<font color='red'>**BIGGEST NOTE**:</font>\n",
    "This entire code is embedded within one giant function because I'm having it run this code repeatedly using different random seeds, until it finds the seed that ensures that ALL participants get assigned time slots. Then it stops and that's the seed number that's kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying seed 1...\n",
      "Success! All participants assigned using seed 1.\n"
     ]
    }
   ],
   "source": [
    "# Define a function to perform the assignment process\n",
    "def assign_slots_with_seed(participants_df, slots_df, seed):\n",
    "\n",
    "    # Add a column to flag timekeepers in the participants dataset\n",
    "    participants_df['is_timekeeper'] = participants_df['Email'].isin(timekeeps)\n",
    "\n",
    "    # Create the blank datasets and lists for the assignments and used-up slots\n",
    "    assignments = []\n",
    "    used_slots = set()\n",
    "\n",
    "    # Add a column to track the number of meetings each participant needs\n",
    "    participants_df['remaining_meetings'] = participants_df.groupby('Email')['Email'].transform('count')\n",
    "\n",
    "    # --- Handle preassigned participants (i.e., participants with requests or who were previously scheduled) ---\n",
    "    preassigned_df = participants_df[participants_df['timeslot_start'].notna()].copy()\n",
    "    to_assign_df = participants_df[participants_df['timeslot_start'].isna()].copy()\n",
    "\n",
    "    for index, participant in preassigned_df.iterrows():\n",
    "        target_time = participant['timeslot_start']\n",
    "\n",
    "        matching_slots = slots_df[\n",
    "            (slots_df['timeslot_start'] == target_time) &\n",
    "            (slots_df['lit_guest_name'].apply(lambda x: set(x) == set(participant['publisher'])))\n",
    "        ]\n",
    "\n",
    "        # Assign to the first available matching slot\n",
    "        for _, slot in matching_slots.iterrows():\n",
    "            room_slot_id = (slot['timeslot_start'], slot['room_name'])\n",
    "            if room_slot_id not in used_slots:\n",
    "                assignments.append({\n",
    "                    'Email': participant['Email'],\n",
    "                    'timeslot_start': slot['timeslot_start'],\n",
    "                    'room_name': slot['room_name'],\n",
    "                    'publisher': slot['lit_guest_name'],\n",
    "                    'virtual': participant['Virtual'],\n",
    "                    'Session': \"Manuscript critique\",\n",
    "                    'Timekeeper': participant['is_timekeeper']\n",
    "                })\n",
    "                used_slots.add(room_slot_id)\n",
    "                break\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: Could not assign preassigned participant {participant['Email']} to their requested timeslot.\")\n",
    "\n",
    "    # Update participants_df to just those who still need assigning\n",
    "    participants_df = to_assign_df.copy()\n",
    "\n",
    "    # --- STEP 2: Continue with normal assignment ---\n",
    "    # Repeat until all participants are assigned or no more slots remain\n",
    "    while not participants_df.empty:\n",
    "        assigned_any = False\n",
    "\n",
    "        for _, slot in slots_df.iterrows():\n",
    "            room_slot_id = (slot['timeslot_start'], slot['room_name'])\n",
    "\n",
    "            if room_slot_id in used_slots:\n",
    "                continue\n",
    "\n",
    "            if participants_df.empty:\n",
    "                break\n",
    "\n",
    "            sorted_participants = (\n",
    "                participants_df\n",
    "                .sample(frac=1, random_state=seed)  # Shuffle randomly\n",
    "                .sort_values(by='remaining_meetings', ascending=False)\n",
    "            )\n",
    "\n",
    "            for index, participant in sorted_participants.iterrows():\n",
    "\n",
    "                # Skip back-to-back assignments\n",
    "                assigned_slots = [\n",
    "                    (a['timeslot_start'], a['room_name']) for a in assignments if a['Email'] == participant['Email']\n",
    "                ]\n",
    "                if any(\n",
    "                    abs(slot['timeslot_start'] - assigned_time) <= timedelta(minutes=15)\n",
    "                    for assigned_time, _ in assigned_slots\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                # Skip the earliest and latest timeslots for timekeepers if possible\n",
    "                if participant['is_timekeeper'] and slot['timeslot_start'] in [earliest_time, latest_time]:\n",
    "                    # Check if there are other slots available for this participant\n",
    "                    has_alternative = any(\n",
    "                        set(participant['publisher']) == set(alt_slot['lit_guest_name']) and\n",
    "                        alt_slot['timeslot_start'] not in [earliest_time, latest_time] and\n",
    "                        (alt_slot['timeslot_start'], alt_slot['room_name']) not in used_slots\n",
    "                        for _, alt_slot in slots_df.iterrows()\n",
    "                    )\n",
    "                    if not has_alternative:\n",
    "                        continue\n",
    "\n",
    "                if set(participant['publisher']) == set(slot['lit_guest_name']):\n",
    "                    assignments.append({\n",
    "                        'Email': participant['Email'],\n",
    "                        'timeslot_start': slot['timeslot_start'],\n",
    "                        'room_name': slot['room_name'],\n",
    "                        'publisher': slot['lit_guest_name'],\n",
    "                        'virtual': participant['Virtual'],\n",
    "                        'Session': \"Manuscript critique\",\n",
    "                        'Timekeeper': participant['is_timekeeper']\n",
    "                    })\n",
    "                    used_slots.add(room_slot_id)\n",
    "                    participants_df.drop(index, inplace=True)\n",
    "                    participants_df['remaining_meetings'] = participants_df.groupby('Email')['Email'].transform('count')\n",
    "                    assigned_any = True\n",
    "                    break\n",
    "\n",
    "        if not assigned_any:\n",
    "            break\n",
    "\n",
    "    return assignments, participants_df\n",
    "\n",
    "# Initialize variables\n",
    "success = False\n",
    "max_attempts = 1000  # Limit the number of attempts\n",
    "seed = 0\n",
    "\n",
    "while not success and seed < max_attempts:\n",
    "    seed += 1\n",
    "    print(f\"Trying seed {seed}...\")\n",
    "    \n",
    "    # Copy the original dataframes to avoid modifying them directly\n",
    "    participants_copy = ms_all_withrequests.copy()\n",
    "    slots_copy = tslist_satmorn.copy()\n",
    "\n",
    "    # Get the earliest and latest timeslots\n",
    "    earliest_time = slots_copy['timeslot_start'].min()\n",
    "    latest_time = slots_copy['timeslot_start'].max()\n",
    "\n",
    "    # Run the assignment process with the current seed\n",
    "    assignments, remaining_participants = assign_slots_with_seed(participants_copy, slots_copy, seed)\n",
    "\n",
    "    # Check if all participants were assigned\n",
    "    if remaining_participants.empty:\n",
    "        success = True\n",
    "        print(f\"Success! All participants assigned using seed {seed}.\")\n",
    "        break\n",
    "\n",
    "if success:\n",
    "    # Convert assignments to a DataFrame\n",
    "    ms_assignments = pd.DataFrame(assignments)\n",
    "else:\n",
    "    print(\"Failed to assign all participants within the maximum number of attempts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! This code works beautifully!! Everyone's been assigned and now let's just do a little cleaning, then repeat the process for the Saturday afternoon pitches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_satmorn_assignments = ms_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just add in the first and last names, plus phones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_satmorn_assignments2 = pd.merge(final_satmorn_assignments, registered[['Email', 'First Name', 'Last Name', 'phone']].drop_duplicates(), on='Email', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del(assignments_df, ms_all, ms_critiques, participants_copy, slots_copy, remaining_participants,\n",
    "    assignments, earliest_time, latest_time, seed, success, assign_slots_with_seed, final_friday_assignments, final_satmorn_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#  Save the dataset\n",
    "final_satmorn_assignments2.to_excel(f\"{current_conference_folder}/Outputs/Finalized datasets/Final manuscript critique assignments_{today}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saturday Afternoon - Pitches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the pitch assignments now! We'll do the exact same process, except using the saturday afternoon times and the pitch dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pitchA = pitches[['Email', 'Virtual', 'pitchA']]\n",
    "pitchB = pitches[['Email', 'Virtual', 'pitchB']]\n",
    "pitchC = pitches[['Email', 'Virtual', 'pitchC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pitchA = pitchA.rename(columns={'pitchA': 'publisher'})\n",
    "pitchB = pitchB.rename(columns={'pitchB': 'publisher'})\n",
    "pitchC = pitchC.rename(columns={'pitchC': 'publisher'})\n",
    "\n",
    "pitches_all = pd.merge(pd.merge(pitchA, pitchB, on=['Email', 'Virtual', 'publisher'], how=\"outer\"), pitchC, on=['Email', 'Virtual', 'publisher'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pitches_all = pitches_all.dropna()\n",
    "del(pitchA, pitchB, pitchC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the code, let's add in the requests (assuming there are any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Now let's merge it with the filtered_pitch_allrequests dataset (assuming it exists)\n",
    "if 'filtered_pitch_allrequests' in locals() and not filtered_pitch_allrequests.empty: # We only want to run this code IF the filtered_pitch_allrequests dataset exists\n",
    "    pitches_all_withrequests = pd.merge(pitches_all, filtered_pitch_allrequests, on=['Email', 'publisher'], how='left').drop_duplicates()\n",
    "else:\n",
    "    pitches_all_withrequests = pitches_all\n",
    "    pitches_all_withrequests['timeslot_start'] = pd.NaT # Only thing you can request is the timeslot, since you paid for a pitch with a specific publisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it gets closer to the conference and people get their MS critiques back, some of them decide not to meet with the editor/agent, and so we can then fill their vacant MS spots with Pitches. We have to keep a manual list of the people we're adding when and when:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pitches_as_ms = pd.read_excel(f'{current_conference_folder}/Hardcode_pitches_slotted_to_MS.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Remove participants who signed up for pitches during the MS slots\n",
    "matches = pitches_all_withrequests.merge(\n",
    "    pitches_as_ms[['Email', 'publisher']],\n",
    "    on=['Email', 'publisher'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Drop those matching rows from pitches_all_withrequests\n",
    "pitches_all_withrequests = pitches_all_withrequests[\n",
    "    ~pitches_all_withrequests.set_index(['Email', 'publisher']).index.isin(\n",
    "        matches.set_index(['Email', 'publisher']).index\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying seed 1...\n",
      "Success! All participants assigned using seed 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-670>:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "# Get the earliest and latest timeslots\n",
    "earliest_time = tslist_sataft['timeslot_start'].min()\n",
    "latest_time = tslist_sataft['timeslot_start'].max()\n",
    "\n",
    "# Define a function to perform the assignment process\n",
    "def assign_slots_with_seed(participants_df, slots_df, seed):\n",
    "\n",
    "    # Shuffle the timeslots within each publisher group using the seed\n",
    "    shuffled_slots = (\n",
    "        slots_df.groupby('lit_guest_name', group_keys=False)\n",
    "        .apply(lambda group: group.sample(frac=1, random_state=seed))\n",
    "    )\n",
    "\n",
    "    # Add a column to flag timekeepers in the participants dataset\n",
    "    participants_df['is_timekeeper'] = participants_df['Email'].isin(timekeeps)\n",
    "\n",
    "    # Create the blank datasets and lists for the assignments and used-up slots\n",
    "    assignments = []\n",
    "    used_slots = set()\n",
    "\n",
    "    # Add a column to track the number of meetings each participant needs\n",
    "    participants_df['remaining_meetings'] = participants_df.groupby('Email')['Email'].transform('count')\n",
    "\n",
    "    # --- Handle any preassigned participants (i.e., those with requests or those previously assigned)\n",
    "    preassigned_df = participants_df[participants_df['timeslot_start'].notna()].copy()\n",
    "    to_assign_df = participants_df[participants_df['timeslot_start'].isna()].copy()\n",
    "\n",
    "    for index, participant in preassigned_df.iterrows():\n",
    "        target_time = participant['timeslot_start']\n",
    "\n",
    "        matching_slots = slots_df[\n",
    "            (slots_df['timeslot_start'] == target_time) &\n",
    "            (slots_df['lit_guest_name'].apply(lambda x: set(x) == set(participant['publisher'])))\n",
    "        ]\n",
    "\n",
    "        # Assign to the first available matching slot\n",
    "        for _, slot in matching_slots.iterrows():\n",
    "            room_slot_id = (slot['timeslot_start'], slot['room_name'])\n",
    "            if room_slot_id not in used_slots:\n",
    "                assignments.append({\n",
    "                    'Email': participant['Email'],\n",
    "                    'timeslot_start': slot['timeslot_start'],\n",
    "                    'room_name': slot['room_name'],\n",
    "                    'publisher': slot['lit_guest_name'],\n",
    "                    'virtual': participant['Virtual'],\n",
    "                    'Session': \"Pitch\",\n",
    "                    'Timekeeper': participant['is_timekeeper']\n",
    "                })\n",
    "                used_slots.add(room_slot_id)\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Warning: Could not assign preassigned participant {participant['Email']} to their requested timeslot.\")\n",
    "\n",
    "    # Update participants_df to just those who still need assigning\n",
    "    participants_df = to_assign_df.copy()\n",
    "\n",
    "    # --- For all participants WITHOUT requests/prior scheduling assignments ---\n",
    "    while not participants_df.empty:\n",
    "        assigned_any = False\n",
    "\n",
    "        for _, slot in shuffled_slots.iterrows():\n",
    "            room_slot_id = (slot['timeslot_start'], slot['room_name'])\n",
    "\n",
    "            if room_slot_id in used_slots:\n",
    "                continue\n",
    "\n",
    "            if participants_df.empty:\n",
    "                break\n",
    "\n",
    "            sorted_participants = (\n",
    "                participants_df\n",
    "                .sample(frac=1, random_state=seed)  # Shuffle randomly\n",
    "                .sort_values(by='remaining_meetings', ascending=False)\n",
    "            )\n",
    "\n",
    "            for index, participant in sorted_participants.iterrows():\n",
    "\n",
    "                # Skip back-to-back assignments\n",
    "                assigned_slots = [\n",
    "                    (a['timeslot_start'], a['room_name']) for a in assignments if a['Email'] == participant['Email']\n",
    "                ]\n",
    "                if any(\n",
    "                    abs(slot['timeslot_start'] - assigned_time) <= timedelta(minutes=15)\n",
    "                    for assigned_time, _ in assigned_slots\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                # Skip the earliest and latest timeslots for timekeepers if possible\n",
    "                if participant['is_timekeeper'] and slot['timeslot_start'] in [earliest_time, latest_time]:\n",
    "                    # Check if there are other slots available for this participant\n",
    "                    if not any(\n",
    "                        set(participant['publisher']) == set(alt_slot['lit_guest_name']) and\n",
    "                        alt_slot['timeslot_start'] not in [earliest_time, latest_time] and\n",
    "                        (alt_slot['timeslot_start'], alt_slot['room_name']) not in used_slots\n",
    "                        for _, alt_slot in shuffled_slots.iterrows()\n",
    "                    ):\n",
    "                        print(f\"Timekeeper {participant['Email']} has no alternative slot; assigning to edge slot.\")\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                if set(participant['publisher']) == set(slot['lit_guest_name']):\n",
    "                    assignments.append({\n",
    "                        'Email': participant['Email'],\n",
    "                        'timeslot_start': slot['timeslot_start'],\n",
    "                        'room_name': slot['room_name'],\n",
    "                        'publisher': slot['lit_guest_name'],\n",
    "                        'virtual': participant['Virtual'],\n",
    "                        'Session': \"Pitch\",\n",
    "                        'Timekeeper': participant['is_timekeeper']\n",
    "                    })\n",
    "                    used_slots.add(room_slot_id)\n",
    "                    participants_df.drop(index, inplace=True)\n",
    "                    participants_df['remaining_meetings'] = participants_df.groupby('Email')['Email'].transform('count')\n",
    "                    assigned_any = True\n",
    "                    break\n",
    "\n",
    "        if not assigned_any:\n",
    "            break\n",
    "\n",
    "    return assignments, participants_df\n",
    "\n",
    "# Initialize variables\n",
    "success = False\n",
    "max_attempts = 1000  # Limit the number of attempts\n",
    "seed = 0\n",
    "\n",
    "while not success and seed < max_attempts:\n",
    "    seed += 1\n",
    "    print(f\"Trying seed {seed}...\")\n",
    "    \n",
    "    # Copy the original dataframes to avoid modifying them directly\n",
    "    participants_copy = pitches_all_withrequests.copy()\n",
    "    slots_copy = tslist_sataft.copy()\n",
    "\n",
    "    # Run the assignment process with the current seed\n",
    "    assignments, remaining_participants = assign_slots_with_seed(participants_copy, slots_copy, seed)\n",
    "\n",
    "    # Check if all participants were assigned\n",
    "    if remaining_participants.empty or len(remaining_participants)==1:\n",
    "        success = True\n",
    "        print(f\"Success! All participants assigned using seed {seed}.\")\n",
    "        break\n",
    "\n",
    "if success:\n",
    "    # Convert assignments to a DataFrame\n",
    "    assignments_df = pd.DataFrame(assignments)\n",
    "else:\n",
    "    print(\"Failed to assign all participants within the maximum number of attempts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! That worked great too. Let's just save it and delete any extraneous datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_sataft_assignment = assignments_df\n",
    "final_sataft_assignments2 = pd.merge(final_sataft_assignment, registered[['Email', 'First Name', 'Last Name', 'phone']].drop_duplicates(), on='Email', how='inner')\n",
    "\n",
    "del(pitches, pitches_all, assignments_df, remaining_participants, timeslots, tslist_sataft, tslist_satmorn, times_friday2, times_sat,\n",
    "    slots_copy, assignments, earliest_time, latest_time, max_attempts, success, seed, assign_slots_with_seed, timedelta, participants_copy, final_sataft_assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! Now we're officially all done with the assignments, and we just need to deal withe waitlists now. FInal step after that will be to print out everything we've got into exactly the excel and word files we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "final_sataft_assignments2.to_excel(f\"{current_conference_folder}/Outputs/Finalized datasets/Finalized pitch assignments_{today}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly,  let's add back in anybody in the pitches as MS hardcodes excel file to the final MS assignments. We'll overwrite it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_satmorn_assignments3 = pd.concat([final_satmorn_assignments2, pitches_as_ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_satmorn_assignments3.to_excel(f\"{current_conference_folder}/Outputs/Finalized datasets/Final manuscript critique assignments_{today}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deal with the Waitlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with the waitlists is pretty simple. We already corrected some of the basic stuff earlier, like emails and phones. Now let's split into what they're waitlisted for:\n",
    "1) manuscript critiques\n",
    "2) pitches\n",
    "3) pre-conference edits\n",
    "4) book fairs\n",
    "5) query letter critiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "wait_ms = waitlist[waitlist['Session Name'].str.contains('Manuscript')].copy()\n",
    "wait_pitch = waitlist[waitlist['Session Name'].str.contains('Pitch')].copy()\n",
    "wait_prec = waitlist[waitlist['Session Name'].str.contains('Pre-conference')].copy()\n",
    "wait_book = waitlist[waitlist['Session Name'].str.contains('Book Fair')].copy()\n",
    "wait_qlc = waitlist[waitlist['Session Name'].str.contains('Query')].copy()\n",
    "wait_coach = waitlist[waitlist['Session Name'].str.contains('oaching')].copy()\n",
    "\n",
    "wait_ms['Waitlist_type'] = 'Manuscript critique'\n",
    "wait_pitch['Waitlist_type'] = 'Pitch'\n",
    "wait_prec['Waitlist_type'] = 'Preconference edit'\n",
    "wait_book['Waitlist_type'] = 'Book fair'\n",
    "wait_qlc['Waitlist_type'] = 'Query letter critique'\n",
    "wait_coach['Waitlist_type'] = 'Author coaching'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Create a publisher variable that for MS and pitch\n",
    "wait_pitch['wl_publisher'] = wait_pitch['Session Name'].str.replace(r\"Pitch [A-Z] with \", \"\", regex=True)\n",
    "wait_ms['wl_publisher'] = wait_ms['Session Name'].str.replace(r\"Manuscript Critique [A-Z] with \", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicates (some people went crazy and signed up for the same waitlist spot)\n",
    "wait_ms = wait_ms.drop_duplicates(subset=['Email', 'wl_publisher', 'Waitlist_type'])\n",
    "wait_pitch = wait_pitch.drop_duplicates(subset=['Email', 'wl_publisher', 'Waitlist_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the A, B, C, etc. from the Session Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# For Pitch sessions\n",
    "wait_pitch.loc[wait_pitch['Session Name'].str.contains(r\"Pitch [A-Z] with \"), 'Session Name'] = \\\n",
    "    wait_pitch['Session Name'].str.replace(r\"Pitch [A-Z] with \", \"Pitch with \", regex=True)\n",
    "\n",
    "# For Manuscript Critique sessions\n",
    "wait_ms.loc[wait_ms['Session Name'].str.contains(r\"Manuscript Critique [A-Z] with \"), 'Session Name'] = \\\n",
    "    wait_ms['Session Name'].str.replace(r\"Manuscript Critique [A-Z] with \", \"Manuscript Critique with \", regex=True)\n",
    "\n",
    "# For Book Fair\n",
    "wait_book.loc[wait_book['Session Name'].str.contains(\"Book Fair Book Selling\"), 'Session Name'] = \\\n",
    "    wait_book['Session Name'].str.replace(\"Book Fair Book Selling\", \"Book Fair Selling\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# We need to link the pitch waitlists to the query letter, pitch and manuscript assignments to drop people whose publisher request is already being met\n",
    "# ------ QLC\n",
    "# Melt to long format and drop NAs\n",
    "qlc_long = pd.melt(final_friday_assignments2, id_vars='Email', value_vars=['pubname1', 'pubname2'])\n",
    "qlc_long = qlc_long.rename(columns={'value': 'qlc_pub'})\n",
    "qlc_long = qlc_long.dropna(subset=['qlc_pub']).drop_duplicates()\n",
    "\n",
    "# Group by Email and assign a number to each pub (for pivoting)\n",
    "qlc_long['pub_num'] = qlc_long.groupby('Email').cumcount() + 1\n",
    "qlc_wide = qlc_long.pivot(index='Email', columns='pub_num', values='qlc_pub')\n",
    "qlc_wide.columns = [f'qlc_pub{i}' for i in qlc_wide.columns]\n",
    "qlc_wide = qlc_wide.reset_index()\n",
    "\n",
    "# ----- Manuscripts\n",
    "final_satmorn_assignments2['pub_num'] = final_satmorn_assignments2.groupby('Email').cumcount() + 1\n",
    "ms_wide = final_satmorn_assignments2[['Email', 'publisher', 'pub_num']].pivot(index='Email', columns='pub_num', values='publisher')\n",
    "ms_wide.columns = [f'ms_pub{i}' for i in ms_wide.columns]\n",
    "ms_wide = ms_wide.reset_index()\n",
    "\n",
    "# ----- Pitches\n",
    "final_sataft_assignments2['pub_num'] = final_sataft_assignments2.groupby('Email').cumcount() + 1\n",
    "pitch_wide = final_sataft_assignments2[['Email', 'publisher', 'pub_num']].pivot(index='Email', columns='pub_num', values='publisher')\n",
    "pitch_wide.columns = [f'pitch_pub{i}' for i in pitch_wide.columns]\n",
    "pitch_wide = pitch_wide.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Now merge it in with the waitlists\n",
    "wait_pitch2 = pd.merge(pd.merge(pd.merge(wait_pitch, qlc_wide, on='Email', how='left'), ms_wide, on='Email', how='left'), pitch_wide, on='Email', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Now filter it out so anybody whose waitlisted pitch person is already being met in the other activities gets removed\n",
    "filter_cols = ['qlc_pub1', 'qlc_pub2', 'qlc_pub3', 'qlc_pub4', 'ms_pub1', 'ms_pub2', 'ms_pub3', 'pitch_pub1', 'pitch_pub2', 'pitch_pub3']\n",
    "\n",
    "wait_pitch_cleaned = wait_pitch2[\n",
    "    ~wait_pitch2.apply(lambda row: row['wl_publisher'] in row[filter_cols].values, axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to double check that no participant has more than 3 manuscript critique or pitch waitlist spots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 1]\n",
      "[3 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(wait_pitch_cleaned['Email'].value_counts().unique())\n",
    "print(wait_ms['Email'].value_counts().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. As you can see above, nobody's got 4 or higher for how often their emails appear in these lists. Now let's sort by registration date for each Session Name, so that we assign a value of #1, #2, etc. by registration date for each Manuscript critique/pitch spot with each publisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Sort by 'Session Name' and 'datetime', and rank participants\n",
    "wait_ms['Waitlist_ms'] = wait_ms.sort_values(['Session Name', 'datetime']) \\\n",
    "               .groupby('Session Name')['datetime'] \\\n",
    "               .rank(method='first').astype(int)\n",
    "\n",
    "# Sort DataFrame for display (optional)\n",
    "wait_ms = wait_ms.sort_values(['Session Name', 'Waitlist_ms']).reset_index(drop=True)\n",
    "\n",
    "# Now add in the variable taht combines the the rank and session field:\n",
    "wait_ms['Waitlisted Activity'] = \"Waitlist #\" + wait_ms['Waitlist_ms'].astype(str) + \" - \" + wait_ms['Session Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-687>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Sort by 'Session Name' and 'datetime', and rank participants\n",
    "wait_pitch_cleaned['Waitlist_pitch'] = wait_pitch_cleaned.sort_values(['Session Name', 'datetime']) \\\n",
    "               .groupby('Session Name')['datetime'] \\\n",
    "               .rank(method='first').astype(int)\n",
    "\n",
    "# Sort DataFrame for display (optional)\n",
    "wait_pitch_cleaned = wait_pitch_cleaned.sort_values(['Session Name', 'Waitlist_pitch']).reset_index(drop=True)\n",
    "\n",
    "# Now add in the variable taht combines the the rank and session field:\n",
    "wait_pitch_cleaned['Waitlisted Activity'] = \"Waitlist #\" + wait_pitch_cleaned['Waitlist_pitch'].astype(str) + \" - \" + wait_pitch_cleaned['Session Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, looks good. Now let's tweak it a little bit more so we create an 'Agenda Item Name' that is 'Waitlisted - #1 - Manuscript Critique with [publisher].'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# For the book fair, just add a simple ranking, according to who registered for the waitlist first\n",
    "wait_book['Waitlist_bookfair'] = wait_book['datetime'].rank(method='first', ascending=True).astype(int)\n",
    "\n",
    "wait_book['Waitlisted Activity'] = \"Waitlist #\" + wait_book['Waitlist_bookfair'].astype(str) + \" - \" + wait_book['Session Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# For the QLC, do a simple ranking - and don't allow any duplicates!!\n",
    "wait_qlc = wait_qlc.drop_duplicates(subset='Email')\n",
    "\n",
    "wait_qlc['Waitlisted_qlc'] = wait_qlc['datetime'].rank(method='first', ascending=True).astype(int)\n",
    "wait_qlc['Waitlisted Activity'] = \"Waitlist #\" + wait_qlc['Waitlisted_qlc'].astype(str) + \" - \" + wait_qlc['Waitlist_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Waitlisted coaching\n",
    "wait_coach = wait_coach.drop_duplicates(subset=['Email', 'Session Name']).sort_values(['Session Name', 'datetime'])\n",
    "wait_coach['Waitlisted_coach']= wait_coach.groupby('Session Name')['datetime'].rank(method='first', ascending=True).astype(int)\n",
    "wait_coach['Waitlisted Activity'] = \"Waitlist #\" + wait_coach['Waitlisted_coach'].astype(str) + \" - \" + wait_coach['Session Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# For right now, let's just merge all the waitlist stuff back together, and add in the participant info so that it's all in one place.\n",
    "wait_all = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(wait_ms[['Email', 'First Name', 'Last Name', 'phone', 'virtual', 'Session Name', 'Waitlisted Activity']], \n",
    "                            wait_pitch_cleaned[['Email', 'First Name', 'Last Name', 'phone', 'virtual', 'Session Name', 'Waitlisted Activity']], how=\"outer\"), \n",
    "                            wait_book[['Email', 'First Name', 'Last Name', 'phone', 'virtual', 'Session Name', 'Waitlisted Activity']], how='outer'),\n",
    "                            wait_qlc[['Email', 'First Name', 'Last Name', 'phone', 'virtual', 'Session Name', 'Waitlisted Activity']], how='outer'),\n",
    "                            wait_book[['Email', 'First Name', 'Last Name', 'phone', 'virtual', 'Session Name', 'Waitlisted Activity']], how='outer'),\n",
    "                            wait_coach[['Email', 'First Name', 'Last Name', 'phone', 'virtual', 'Session Name', 'Waitlisted Activity']], how='outer')\n",
    "\n",
    "# print for George\n",
    "wait_all.to_excel(f\"{current_conference_folder}/Outputs/Finalized Datasets/Waitlist participants_{today}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't really do much with these particular excel documents, except to export them for manual review (and potentially manual changes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "registered.to_excel(f\"{current_conference_folder}/Outputs/Finalized Datasets/Registered_cleaned_{today}.xlsx\", index=False,\n",
    "                        columns=['Agenda Item Name', 'Email', 'First Name', 'Last Name', 'Mobile Phone Number', 'Virtual', 'Fiction genre', 'Nonfiction genre', 'phone'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "waitlist.to_excel(f\"{current_conference_folder}/Outputs/Finalized Datasets/Waitlist_cleaned_{today}.xlsx\", index=False,\n",
    "                        columns=['Session Name', 'Email', 'First Name', 'Last Name', 'Mobile Phone Number', 'phone', 'virtual'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify all participants who match each publisher\n",
    "\n",
    "To make it easier to figure out who to email about open pitch or QLC spots (or open MS critiques that will be used as pitch spots), we're going to make a list of all waitlisted and registered individuals and all their activities, merge it with the info on their genres, and then iterate through the publishers' genres to identify everyone they'd match with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataset with one row per registered and waitlisted participants with all their activities\n",
    "everyone = pd.merge(pd.merge(pd.merge(registered[['Email', 'First Name', 'Last Name', 'phone', 'Fiction genre', 'Nonfiction genre']], qlc_wide, on='Email', how='outer'),\n",
    "                    ms_wide, on='Email', how='outer'), pitch_wide, on='Email', how='outer')\n",
    "everyone = everyone.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "everyone['Nonfiction genre'] = everyone['Nonfiction genre'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])\n",
    "everyone['Fiction genre'] = everyone['Fiction genre'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Now fix the publishers' genres\n",
    "pubs['lit_guest_fiction'] = pubs['lit_guest_fiction'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])\n",
    "pubs['lit_guest_nonfiction'] = pubs['lit_guest_nonfiction'].apply(lambda x: [genre.strip() for genre in x.split(',')] if isinstance(x, str) else [genre.strip() for genre in x] if isinstance(x, list) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def clean_split(genres):\n",
    "    # Safely split, strip, and lowercase genres\n",
    "    return set(g.strip().lower() for g in str(genres).split(',') if g.strip())\n",
    "\n",
    "def find_matching_publishers(row):\n",
    "    p_fiction = clean_split(row['Fiction genre'])\n",
    "    p_nonfiction = clean_split(row['Nonfiction genre'])\n",
    "\n",
    "    columns = ['qlc_pub1', 'qlc_pub2', 'qlc_pub3', 'qlc_pub4',\n",
    "               'ms_pub1', 'ms_pub2', 'ms_pub3',\n",
    "               'pitch_pub1', 'pitch_pub2', 'pitch_pub3']\n",
    "    scheduled = set(str(row[col]).strip().lower() for col in columns if pd.notna(row[col]))\n",
    "\n",
    "    matching = []\n",
    "    for _, pub in pubs.iterrows():\n",
    "        pub_name = str(pub['lit_guest_name']).strip()\n",
    "        pub_name_lower = pub_name.lower()\n",
    "\n",
    "        pub_fiction = clean_split(pub['lit_guest_fiction'])\n",
    "        pub_nonfiction = clean_split(pub['lit_guest_nonfiction'])\n",
    "\n",
    "        if (p_fiction & pub_fiction or p_nonfiction & pub_nonfiction) and pub_name_lower not in scheduled:\n",
    "            matching.append(pub_name)\n",
    "\n",
    "    return ', '.join(matching)\n",
    "\n",
    "# Now apply to the participants dataframe\n",
    "everyone['matching_publishers'] = everyone.apply(find_matching_publishers, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "everyone.to_excel(f\"{current_conference_folder}/Outputs/Finalized Datasets/All_participants_with_matching_publishers_{today}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. To run when Manuscript Critique submissions have passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manuscript critiques are due one month prior to the conference. After that point, anyone on a waitlist for the manuscript critiques can no longer be moved off the waitlist.\n",
    "\n",
    "However, those participants might want instead to sign up for a query critique with the people whose manuscript critique waitlists they didn't get.\n",
    "\n",
    "To do that, we'll:\n",
    "\n",
    "1. Check how many pitches and query letter critique spots are still open, and who they're with\n",
    "2. See which participants have:\n",
    "    * Manuscript critique waitlist spots\n",
    "    * Aren't yet registered for a query letter critique \n",
    "    * Don't have any registered pitch spots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publisher\n",
       "Kurt Brackob           False\n",
       "Monica Rae Brown       False\n",
       "Paloma Hernando        False\n",
       "Nicole Luongo          False\n",
       "Renée Fountain         False\n",
       "Dianna Vega            False\n",
       "Micah Brocker          False\n",
       "Wendy Wong             False\n",
       "Joëlle Delbourgo       False\n",
       "Alexandria Brown       False\n",
       "Foyinsi Adegbonmire    False\n",
       "Jynastie Wilson        False\n",
       "Lauren Bieker          False\n",
       "Jéla Lewter            False\n",
       "Jenna Satterthwaite    False\n",
       "Grace Gay              False\n",
       "Vicky Weber            False\n",
       "Jake Lovell            False\n",
       "Name: count, dtype: bool"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get value_counts for each of the pitches to see which publishers have openings\n",
    "final_sataft_assignments2['publisher'].value_counts()<12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Check the QLC openings\n",
    "qlc_openings = pd.DataFrame(final_friday_assignments2['publisher'].value_counts())\n",
    "qlc_openings = qlc_openings.loc[qlc_openings['count']<12,]\n",
    "qlc_openings['Pairings'] = qlc_openings.index\n",
    "qlc_openings['Openings'] = 12-qlc_openings['count']\n",
    "qlc_openings = qlc_openings.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Pairings, Openings]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(qlc_openings[['Pairings', 'Openings']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "qlc_openings.to_excel(f\"{current_conference_folder}/Outputs/Finalized Datasets/Open_Query_spots_{today}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's dig into the participants themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "wait_toemail = wait_ms[['Email', 'First Name', 'Last Name', 'phone', 'virtual', 'Session Name']].copy()\n",
    "wait_toemail['Publisher'] = wait_toemail['Session Name'].str.removeprefix(\"Waitlisted - Manuscript Critique with \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Link this list to the pitches and QLCs\n",
    "wait_toemail2 = pd.merge(pd.merge(wait_toemail, final_sataft_assignments2[['Email', 'publisher']], on='Email', how='outer'), \n",
    "                            final_friday_assignments2[['Email', 'pubname1', 'pubname2']], on='Email', how='outer')\n",
    "wait_toemail2 = wait_toemail2.loc[wait_toemail2['Publisher'].notna(),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# The above created a big list, with multiple rows per participant. We're going to find all the rows where the participant's \n",
    "# requested publisher for the manuscript critique has already been fulfilled via a pitch or QLC. Then we'll drop ALL their emails from the toemail\n",
    "# dataset so we only identify people with unmet needs.\n",
    "\n",
    "wait_toemail2['Fulfilled'] = wait_toemail2.apply(\n",
    "    lambda row: row['Publisher'] in [row['publisher'], row['pubname1'], row['pubname2']], axis=1\n",
    ")\n",
    "\n",
    "fulfilled = wait_toemail2.loc[wait_toemail2['Fulfilled']== True, ]\n",
    "unfulfilled = wait_toemail2.loc[~wait_toemail2['Email'].isin(fulfilled['Email']), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Okay, drop any duplicates now so we just have each person and the people they requested for manuscript critiques that they haven't already fulfill\n",
    "# via a pitch or QLC. We also need to remove anyone who requested Alexandria brown or renee fountain, since their QLC spots are all taken\n",
    "\n",
    "wait_toemail = unfulfilled[['Email', 'First Name', 'Last Name', 'Publisher', 'virtual']].drop_duplicates(subset=['Email', 'Publisher'])\n",
    "wait_toemail = wait_toemail.loc[wait_toemail['Publisher'].str.contains('Vicky Weber|Wendy Wong|Lauren Bieker|Nicole Luongo|Alexandria Brown|Renée Fountain|Jynastie Wilson|Dianna Vega'),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Print this to excel\n",
    "wait_toemail.to_excel(f\"{current_conference_folder}/Outputs/Finalized Datasets/MSwaitlists_unfulfilled_{today}.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
